import os
import json
import re
import hashlib
import pickle
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
import logging

import feedparser
import requests
from bs4 import BeautifulSoup
import trafilatura
from openai import OpenAI
from dotenv import load_dotenv

import lark_oapi as lark
from lark_oapi.api.docx.v1 import (
    CreateDocumentRequest,
    CreateDocumentRequestBody,
    ConvertDocumentRequest,
    ConvertDocumentRequestBody,
    TextElement,
    TextRun,
    TextElementStyle,
    Link,
    UpdateBlockRequest,
    BatchUpdateDocumentBlockRequest,
    BatchUpdateDocumentBlockRequestBody,
    CreateDocumentBlockChildrenRequest,
    CreateDocumentBlockChildrenRequestBody,
    Block,
    Text as TextModel
)
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody
from lark_oapi.api.drive.v1 import (
    CreatePermissionMemberRequest,
    BaseMember,
    BatchCreatePermissionMemberRequest,
    BatchCreatePermissionMemberRequestBody
)

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class NewsConfig:
    """
    æ–°é—»æ±‡æ€»é…ç½®ç±»
    
    ç”¨äºå°è£…ä¸åŒç±»å‹æ–°é—»æ±‡æ€»çš„å¯é…ç½®å‚æ•°ï¼Œæ”¯æŒçµæ´»æ‰©å±•å¤šç§æ–°é—»ç±»å‹ã€‚
    
    Attributes:
        name: é…ç½®åç§°æ ‡è¯†ï¼ˆå¦‚ "outdoor_sports", "tech_news"ï¼‰
        target_sites: ç›®æ ‡ç½‘ç«™URLåˆ—è¡¨
        rss_feeds: RSSæºæ˜ å°„å­—å…¸ {site_url: rss_url}
        ai_prompt: AIåˆ†æpromptæ¨¡æ¿
        ai_system_prompt: AIç³»ç»Ÿprompt
        feishu_collaborator_openids: é£ä¹¦åä½œè€…openidåˆ—è¡¨
        report_title_template: æ–°é—»æ±‡æ€»æ ‡é¢˜æ¨¡æ¿ï¼Œæ”¯æŒ {start_date} å’Œ {end_date} å ä½ç¬¦
        report_header: æ–°é—»æ±‡æ€»æ ‡é¢˜ï¼ˆMarkdownæ ¼å¼ï¼‰
        cache_prefix: ç¼“å­˜å‰ç¼€ï¼Œç”¨äºåŒºåˆ†ä¸åŒç±»å‹æ–°é—»çš„ç¼“å­˜
    """
    name: str
    target_sites: List[str] = field(default_factory=list)
    rss_feeds: Dict[str, str] = field(default_factory=dict)
    ai_prompt: str = ""
    ai_system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿æ‰¹é‡æå–æ–‡ç« å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œä¸­è‹±æ–‡ç¿»è¯‘ã€‚"
    feishu_collaborator_openids: List[str] = field(default_factory=list)
    report_title_template: str = "{name}æ–°é—»æ±‡æ€» ({start_date} è‡³ {end_date})"
    report_header: str = "# æ–°é—»æ±‡æ€»\n"
    cache_prefix: str = ""

# ä¿å­˜åŸå§‹ä»£ç†è®¾ç½®
_original_proxy_settings = {
    'HTTP_PROXY': os.environ.get('HTTP_PROXY'),
    'HTTPS_PROXY': os.environ.get('HTTPS_PROXY'),
    'ALL_PROXY': os.environ.get('ALL_PROXY')
}

# RSSç¼“å­˜é…ç½®
RSS_CACHE_DIR = "cache/rss"
RSS_CACHE_TTL = 3600  # 1å°æ—¶ç¼“å­˜

# HTMLæŠ“å–ç¼“å­˜é…ç½®
HTML_CACHE_DIR = "cache/html"
HTML_CACHE_TTL = 3600 * 6  # 6å°æ—¶ç¼“å­˜

# AIå¤„ç†ç¼“å­˜é…ç½®
AI_CACHE_DIR = "cache/ai"
AI_CACHE_TTL = 86400 * 7  # 7å¤©ç¼“å­˜ï¼ˆAIå¤„ç†ç»“æœé•¿æœŸæœ‰æ•ˆï¼‰

# åˆ›å»ºç¼“å­˜ç›®å½•
os.makedirs(RSS_CACHE_DIR, exist_ok=True)
os.makedirs(HTML_CACHE_DIR, exist_ok=True)
os.makedirs(AI_CACHE_DIR, exist_ok=True)


def clean_expired_cache(cache_dir: str, ttl: int, cache_type: str = "cache"):
    """
    æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        cache_type: ç¼“å­˜ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    if not os.path.exists(cache_dir):
        return
    
    current_time = datetime.now().timestamp()
    cleaned_count = 0
    total_size = 0
    
    try:
        for filename in os.listdir(cache_dir):
            filepath = os.path.join(cache_dir, filename)
            
            if os.path.isfile(filepath):
                file_time = os.path.getmtime(filepath)
                file_age = current_time - file_time
                
                if file_age > ttl:
                    file_size = os.path.getsize(filepath)
                    os.remove(filepath)
                    cleaned_count += 1
                    total_size += file_size
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸ{cache_type}: {filename} (å·²è¿‡æœŸ {file_age // 3600:.1f} å°æ—¶)")
        
        if cleaned_count > 0:
            size_mb = total_size / (1024 * 1024)
            logger.info(f"âœ… {cache_type}æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶, é‡Šæ”¾ {size_mb:.2f} MB")
        else:
            logger.info(f"âœ… {cache_type}æ— éœ€æ¸…ç†: æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æœ‰æ•ˆæœŸå†…")
            
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…ç†{cache_type}å¤±è´¥: {str(e)}")


def clean_all_expired_caches():
    """æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ç¼“å­˜"""
    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜...")
    
    clean_expired_cache(RSS_CACHE_DIR, RSS_CACHE_TTL, "RSSç¼“å­˜")
    clean_expired_cache(HTML_CACHE_DIR, HTML_CACHE_TTL, "HTMLç¼“å­˜")
    clean_expired_cache(AI_CACHE_DIR, AI_CACHE_TTL, "AIç¼“å­˜")
    
    logger.info("ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ")


def get_rss_cache_path(rss_url: str) -> str:
    """è·å–RSSç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨URLçš„hashä½œä¸ºæ–‡ä»¶å
    url_hash = hashlib.md5(rss_url.encode()).hexdigest()
    return os.path.join(RSS_CACHE_DIR, f"{url_hash}.pkl")


def load_rss_from_cache(rss_url: str) -> Optional[feedparser.FeedParserDict]:
    """ä»ç¼“å­˜åŠ è½½RSSæ•°æ®"""
    cache_path = get_rss_cache_path(rss_url)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
        cache_time = os.path.getmtime(cache_path)
        current_time = datetime.now().timestamp()
        
        if current_time - cache_time > RSS_CACHE_TTL:
            logger.info(f"ğŸ“¦ RSSç¼“å­˜è¿‡æœŸ: {rss_url}")
            return None
        
        with open(cache_path, 'rb') as f:
            cached_data = pickle.load(f)
        logger.info(f"ğŸ“¦ RSSç¼“å­˜å‘½ä¸­: {rss_url}")
        return cached_data
        
    except Exception as e:
        logger.warning(f"ğŸ“¦ RSSç¼“å­˜åŠ è½½å¤±è´¥: {rss_url} - {str(e)}")
        return None


def save_rss_to_cache(rss_url: str, feed_data: feedparser.FeedParserDict) -> bool:
    """ä¿å­˜RSSæ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_rss_cache_path(rss_url)
    
    try:
        with open(cache_path, 'wb') as f:
            pickle.dump(feed_data, f)
        logger.info(f"ğŸ“¦ RSSç¼“å­˜ä¿å­˜: {rss_url}")
        return True
    except Exception as e:
        logger.warning(f"ğŸ“¦ RSSç¼“å­˜ä¿å­˜å¤±è´¥: {rss_url} - {str(e)}")
        return False


# ================================
# HTMLæŠ“å–ç¼“å­˜å‡½æ•°
# ================================

def get_html_cache_path(url: str) -> str:
    """è·å–HTMLç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    url_hash = hashlib.md5(url.encode()).hexdigest()
    return os.path.join(HTML_CACHE_DIR, f"{url_hash}.json")


def load_html_from_cache(url: str) -> Optional[str]:
    """ä»ç¼“å­˜åŠ è½½HTMLå†…å®¹"""
    cache_path = get_html_cache_path(url)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        cache_time = os.path.getmtime(cache_path)
        current_time = datetime.now().timestamp()
        
        if current_time - cache_time > HTML_CACHE_TTL:
            logger.info(f"ğŸ“¦ HTMLç¼“å­˜è¿‡æœŸ: {url}")
            return None
        
        with open(cache_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
        logger.info(f"ğŸ“¦ HTMLç¼“å­˜å‘½ä¸­: {url}")
        return cached_data.get('content')
        
    except Exception as e:
        logger.warning(f"ğŸ“¦ HTMLç¼“å­˜åŠ è½½å¤±è´¥: {url} - {str(e)}")
        return None


def save_html_to_cache(url: str, content: str) -> bool:
    """ä¿å­˜HTMLå†…å®¹åˆ°ç¼“å­˜"""
    cache_path = get_html_cache_path(url)
    
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({'content': content, 'timestamp': datetime.now().isoformat()}, f, ensure_ascii=False)
        # logger.info(f"ğŸ“¦ HTMLç¼“å­˜ä¿å­˜: {url}")
        return True
    except Exception as e:
        logger.warning(f"ğŸ“¦ HTMLç¼“å­˜ä¿å­˜å¤±è´¥: {url} - {str(e)}")
        return False


# ================================
# AIå¤„ç†ç¼“å­˜å‡½æ•°
# ================================

def get_ai_cache_path(url: str) -> str:
    """è·å–AIç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨URLçš„hashä½œä¸ºæ–‡ä»¶å
    url_hash = hashlib.md5(url.encode()).hexdigest()
    return os.path.join(AI_CACHE_DIR, f"{url_hash}.json")


def load_ai_from_cache(url: str) -> Optional[Dict]:
    """ä»ç¼“å­˜åŠ è½½AIå¤„ç†ç»“æœ"""
    cache_path = get_ai_cache_path(url)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
        cache_time = os.path.getmtime(cache_path)
        current_time = datetime.now().timestamp()
        
        if current_time - cache_time > AI_CACHE_TTL:
            logger.info(f"ğŸ“¦ AIç¼“å­˜è¿‡æœŸ: {url}")
            return None
        
        with open(cache_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
        # logger.info(f"ğŸ“¦ AIç¼“å­˜å‘½ä¸­: {url}")
        return cached_data
        
    except Exception as e:
        logger.warning(f"ğŸ“¦ AIç¼“å­˜åŠ è½½å¤±è´¥: {url} - {str(e)}")
        return None


def save_ai_to_cache(url: str, ai_result: Dict) -> bool:
    """ä¿å­˜AIå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
    cache_path = get_ai_cache_path(url)
    
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(ai_result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“¦ AIç¼“å­˜ä¿å­˜: {url}")
        return True
    except Exception as e:
        logger.warning(f"ğŸ“¦ AIç¼“å­˜ä¿å­˜å¤±è´¥: {url} - {str(e)}")
        return False


def parse_rss_with_cache(rss_url: str) -> Optional[feedparser.FeedParserDict]:
    """è§£æRSSï¼Œæ”¯æŒç¼“å­˜"""
    # å°è¯•ä»ç¼“å­˜åŠ è½½
    feed = load_rss_from_cache(rss_url)
    if feed:
        return feed
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè§£æRSS
    logger.info(f"ğŸ” è§£æRSS: {rss_url}")
    feed = feedparser.parse(rss_url)
    
    if feed.entries:
        # ä¿å­˜åˆ°ç¼“å­˜
        save_rss_to_cache(rss_url, feed)
    
    return feed


def enable_proxy_for_web_scraping():
    """
    æ¢å¤ä»£ç†è®¾ç½®ï¼ˆç”¨äºç½‘ç«™æŠ“å–ï¼‰
    """
    # æ¢å¤ä»£ç†ç¯å¢ƒå˜é‡
    for key, value in _original_proxy_settings.items():
        if value:
            os.environ[key] = value
    logger.info("ğŸŒ æ¢å¤ä»£ç†è®¾ç½® (ç”¨äºç½‘ç«™æŠ“å–)")

def clear_all_proxy():
    """
    å®Œå…¨æ¸…é™¤æ‰€æœ‰ä»£ç†è®¾ç½®
    """
    # æ¸…é™¤æ‰€æœ‰å¯èƒ½çš„ä»£ç†å˜é‡
    proxy_vars = [
        'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY',
        'http_proxy', 'https_proxy', 'all_proxy',
        'HTTP_PROXY_HOST', 'HTTP_PROXY_PORT',
        'HTTPS_PROXY_HOST', 'HTTPS_PROXY_PORT',
        'NO_PROXY', 'no_proxy'
    ]
    for var in proxy_vars:
        os.environ.pop(var, None)
    
    # ç¦ç”¨å½“å‰è¿›ç¨‹çš„æ‰€æœ‰ç½‘ç»œä»£ç†
    os.environ['NO_PROXY'] = '*'
    os.environ['no_proxy'] = '*'
    
    # æ¸…é™¤requestsçš„ä»£ç†è®¾ç½®
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    session = requests.Session()
    session.trust_env = False
    retry_strategy = Retry(
        total=2,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    logger.info("ğŸš« å®Œå…¨æ¸…é™¤æ‰€æœ‰ä»£ç†è®¾ç½®")


def get_feishu_client():
    """
    è·å–é£ä¹¦å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¸…é™¤ä»£ç†ï¼‰
    """
    clear_all_proxy()
    
    try:
        client = lark.Client.builder() \
            .app_id(os.getenv("FEISHU_APP_ID")) \
            .app_secret(os.getenv("FEISHU_APP_SECRET")) \
            .log_level(lark.LogLevel.INFO) \
            .build()
        
        return client
    except Exception as e:
        logger.error(f"åˆ›å»ºé£ä¹¦å®¢æˆ·ç«¯å¤±è´¥: {e}")
        # å¦‚æœé…ç½®å¤±è´¥ï¼Œåˆ›å»ºæœ€åŸºæœ¬çš„å®¢æˆ·ç«¯
        client = lark.Client.builder() \
            .app_id(os.getenv("FEISHU_APP_ID")) \
            .app_secret(os.getenv("FEISHU_APP_SECRET")) \
            .build()
        return client


def _get_openai_client():
    """
    è·å–OpenAIå®¢æˆ·ç«¯ï¼ˆæ¸…é™¤ä»£ç†ï¼‰
    """
    clear_all_proxy()
    
    api_key = os.getenv('LLM_API_KEY')
    base_url = os.getenv('LLM_BASE_URL')
    
    if not api_key:
        raise ValueError('LLM_API_KEY environment variable is not set')
    
    client_kwargs = {'api_key': api_key}
    if base_url:
        client_kwargs['base_url'] = base_url
    
    return OpenAI(**client_kwargs)

# ä»ç¯å¢ƒå˜é‡è¯»å–ç½‘ç«™åˆ—è¡¨ï¼Œæ ¼å¼ï¼šç”¨é€—å·åˆ†éš”çš„URLåˆ—è¡¨
TARGET_SITES = os.getenv('TARGET_SITES', '').split(',') if os.getenv('TARGET_SITES') else []
TARGET_SITES = [site.strip() for site in TARGET_SITES if site.strip()]

# ä»ç¯å¢ƒå˜é‡è¯»å–RSSæ˜ å°„ï¼Œæ ¼å¼ï¼šsite1_url=rss1_url,site2_url=rss2_url
RSS_FEEDS_ENV = os.getenv('RSS_FEEDS', '')
RSS_FEEDS = {}

if RSS_FEEDS_ENV:
    for mapping in RSS_FEEDS_ENV.split(','):
        if '=' in mapping:
            site_url, rss_url = mapping.split('=', 1)
            RSS_FEEDS[site_url.strip()] = rss_url.strip()

def fetch_outdoor_articles(start_date: date, end_date: date, max_workers: int = 3) -> List[Dict]:
    """
    å¹¶è¡ŒæŠ“å–æˆ·å¤–è¿åŠ¨ç›¸å…³æ–‡ç« ï¼ˆå‘åå…¼å®¹å‡½æ•°ï¼‰
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆä»…ç”¨äºç½‘ç«™çº§å¹¶å‘ï¼‰
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    return fetch_articles(start_date, end_date, max_workers=max_workers)


def fetch_articles(start_date: date, end_date: date, 
                   config: NewsConfig = None,
                   target_sites: List[str] = None,
                   rss_feeds: Dict[str, str] = None,
                   max_workers: int = 3) -> List[Dict]:
    """
    å¹¶è¡ŒæŠ“å–æ–°é—»æ–‡ç« ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
    
    æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
    1. é€šè¿‡ NewsConfig é…ç½®å¯¹è±¡ä¼ å…¥å‚æ•°
    2. ç›´æ¥ä¼ å…¥ target_sites å’Œ rss_feeds å‚æ•°
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        config: NewsConfig é…ç½®å¯¹è±¡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
        target_sites: ç›®æ ‡ç½‘ç«™åˆ—è¡¨ï¼ˆconfig ä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰
        rss_feeds: RSSæºæ˜ å°„ï¼ˆconfig ä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆä»…ç”¨äºç½‘ç«™çº§å¹¶å‘ï¼‰
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    sites = config.target_sites if config else (target_sites or TARGET_SITES)
    feeds = config.rss_feeds if config else (rss_feeds or RSS_FEEDS)
    
    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡ŒæŠ“å–æ–‡ç« : {start_date} åˆ° {end_date}")
    
    enable_proxy_for_web_scraping()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        site_url_map = {}
        for site_url in sites:
            rss_feed = feeds.get(site_url)
            
            if rss_feed:
                future = executor.submit(_fetch_from_rss, rss_feed, site_url, start_date, end_date)
            else:
                future = executor.submit(_fetch_from_html, site_url, start_date, end_date)
            
            futures.append(future)
            site_url_map[id(future)] = site_url
        
        articles = []
        completed = 0
        for future in as_completed(futures):
            try:
                site_result = future.result()
                current_site_url = site_url_map.get(id(future), "æœªçŸ¥ç½‘ç«™")
                
                if isinstance(site_result, dict):
                    site_articles = site_result.get('articles', [])
                    articles.extend(site_articles)
                elif isinstance(site_result, list):
                    articles.extend(site_result)
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥è¿”å›ç±»å‹: {type(site_result)}")
                
                completed += 1
                logger.info(f"âœ… å®Œæˆ {completed}/{len(sites)} ä¸ªç½‘ç«™ï¼š{current_site_url}")
            except Exception as e:
                current_site_url = site_url_map.get(id(future), "æœªçŸ¥ç½‘ç«™")
                logger.error(f"âŒ æŠ“å–ç½‘ç«™å¤±è´¥: {current_site_url} - {str(e)}")
                completed += 1
    
    logger.info(f"ğŸ‰ æ‰€æœ‰ç½‘ç«™æŠ“å–å®Œæˆï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def _clean_rss_content(raw_html: str) -> str:
    # 1. å¤„ç†è½¬ä¹‰å­—ç¬¦ï¼šå°† \\n æ›¿æ¢ä¸ºæ¢è¡Œï¼Œå°† \\\" æ›¿æ¢ä¸ºå¼•å·
    content = raw_html.replace('\\n', '\n').replace('\\"', '"')
    
    # 2. ä½¿ç”¨ BeautifulSoup è§£æ HTML
    soup = BeautifulSoup(content, 'html.parser')
    
    # 3. ç§»é™¤ä¸ç›¸å…³çš„æ ‡ç­¾ï¼ˆå¦‚å›¾ç‰‡è¯´æ˜ã€è„šæœ¬ã€æ ·å¼ï¼‰
    for extra in soup(['figure', 'script', 'style', 'img']):
        extra.decompose()
        
    # 4. è·å–æ–‡æœ¬ï¼Œå¹¶å¤„ç†æ‰ RSS ä¸­å¸¸è§çš„é‡å¤é“¾æ¥ï¼ˆæ¯”å¦‚ "The post...appeared first on..."ï¼‰
    lines = []
    for p in soup.find_all(['p', 'h1', 'h2', 'h3']):
        text = p.get_text().strip()
        # è¿‡æ»¤æ‰ RSS è‡ªåŠ¨ç”Ÿæˆçš„æœ«å°¾æ¨å¹¿è¯­
        if "The post" in text and "appeared first on" in text:
            continue
        if text:
            lines.append(text)
    
    # 5. å»é‡ï¼ˆRSS æœ‰æ—¶ä¼šé‡å¤æ¨é€æ­£æ–‡ç‰‡æ®µï¼‰
    unique_lines = []
    for line in lines:
        if line not in unique_lines:
            unique_lines.append(line)
            
    return "\n\n".join(unique_lines)


def _fetch_from_rss(rss_url: str, site_url: str, start_date: date, end_date: date) -> List[Dict]:
    """
    ä»RSSæºæŠ“å–æ–‡ç« ï¼ˆä¸²è¡Œæ‰§è¡Œï¼Œç®€åŒ–é€»è¾‘ï¼‰
    """
    articles = []
    
    try:
        # ä½¿ç”¨ç¼“å­˜è§£æRSS
        feed = parse_rss_with_cache(rss_url)
        if not feed:
            logger.warning(f"RSSè§£æå¤±è´¥: {rss_url}")
            return articles
            
        logger.info(f"ğŸ” RSS[{rss_url}] ä¸­å…±æœ‰ {len(feed.entries)} æ¡ç›®")
        
        # æ­¥éª¤1: è§£æRSSï¼ˆå¿«é€Ÿï¼Œæœ¬åœ°å¤„ç†ï¼‰
        # æ­¥éª¤2: è¿‡æ»¤æ—¥æœŸèŒƒå›´å¹¶ç›´æ¥æå–RSSå†…å®¹ï¼ˆé¿å…ç½‘é¡µæŠ“å–ï¼‰
        article_data = []
        
        for entry in feed.entries:
            if hasattr(entry, 'published_parsed'):
                article_date = datetime(*entry.published_parsed[:6])
                title = entry.get('title', '')
                
                if start_date <= article_date.date() <= end_date:
                    # æ–‡ç« æ—¥æœŸåœ¨èŒƒå›´å†…ï¼Œç›´æ¥ä»RSSæå–å†…å®¹
                    article_url = entry.get('link', '')
                    
                    # ç›´æ¥ä»RSSæ¡ç›®ä¸­æå–å†…å®¹
                    description = entry.get('description', '')
                    summary = entry.get('summary', '')
                    
                    # å°è¯•è·å–å®Œæ•´çš„æ–‡ç« å†…å®¹
                    content_encoded = ''
                    if entry.get('content'):
                        # feedparserä¼šå°†contentå­—æ®µè§£æä¸ºåˆ—è¡¨
                        content_list = entry.get('content', [])
                        if content_list and len(content_list) > 0:
                            content_encoded = content_list[0].get('value', '')
                    
                    # æ„å»ºå®Œæ•´æ–‡ç« æ•°æ®
                    article_data.append({
                        'title': title,
                        'url': article_url,
                        'date': article_date.date().isoformat(),
                        'site': site_url,
                        'description': description,
                        'summary': summary,
                        'content_encoded': content_encoded,
                        'raw_content': description + ' ' + summary + ' ' + content_encoded
                    })
        
        logger.info(f"ğŸ“… RSS[{rss_url}] æ‰¾åˆ° {len(article_data)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–‡ç« ")
        
        # æ­¥éª¤3: ç›´æ¥ä½¿ç”¨RSSå†…å®¹ï¼Œé¿å…ç½‘é¡µæŠ“å–
        if article_data:
            for data in article_data:
                try:
                    # æ¸…ç†RSSå†…å®¹ï¼Œç§»é™¤HTMLæ ‡ç­¾å’Œå…ƒæ•°æ®
                    content_text = _clean_rss_content(data['raw_content'])
                    
                    if content_text and len(content_text) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                        articles.append({
                            'site': data['site'],
                            'url': data['url'],
                            'title': data['title'],
                            'date': data['date'],
                            'content_text': content_text
                        })
                    else:
                        logger.warning(f"RSSå†…å®¹è´¨é‡è¾ƒå·®: {data['url']} (å†…å®¹é•¿åº¦: {len(content_text)})")
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†RSSå†…å®¹å¤±è´¥: {data['url']} - {str(e)}")
        
    except Exception as e:
        logger.error(f"RSSæŠ“å–å¤±è´¥ {rss_url}: {str(e)}")
    
    return articles


def _fetch_from_html(site_url: str, start_date: date, end_date: date) -> Dict:
    """
    ä»HTMLé¡µé¢æŠ“å–æ–‡ç« ï¼ˆæ”¹è¿›é”™è¯¯å¤„ç†ï¼Œä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼æ•°æ®ï¼‰
    è¿”å›ï¼š{
        'articles': List[Dict],  # æˆåŠŸå¤„ç†çš„æ–‡ç« 
        'failed_articles': List[Dict],  # å¤„ç†å¤±è´¥çš„æ–‡ç« ï¼ˆä¿ç•™åŸºæœ¬ä¿¡æ¯ï¼‰
        'statistics': Dict  # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    }
    """
    result = {
        'articles': [],
        'failed_articles': [],
        'statistics': {
            'total_entries': 0,
            'filtered_by_date': 0,
            'successful_extraction': 0,
            'failed_extraction': 0,
            'error_messages': []
        }
    }
    
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    import ssl
    import urllib3
    
    session = requests.Session()
    
    original_env_backup = os.environ.copy()
    
    BROWSER_HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0',
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"'
    }
    
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    try:
        enable_proxy_for_web_scraping()
        session.trust_env = True
        session.headers.update(BROWSER_HEADERS)
        
        logger.info(f"ğŸŒ å°è¯•é€šè¿‡ä»£ç†è®¿é—®: {site_url}")
        response = session.get(site_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        article_links = _extract_article_links(soup, site_url)
        result['statistics']['total_entries'] = len(article_links)
        
        for link in article_links:
            try:
                content_text = _extract_content_with_session(link, session)
                
                if content_text:
                    result['articles'].append({
                        'site': site_url,
                        'url': link,
                        'title': _extract_title_from_url(link),
                        'content_text': content_text,
                        'date': None
                    })
                    result['statistics']['successful_extraction'] += 1
                else:
                    result['failed_articles'].append({
                        'site': site_url,
                        'url': link,
                        'title': _extract_title_from_url(link),
                        'error': 'å†…å®¹æå–å¤±è´¥',
                        'date': None
                    })
                    result['statistics']['failed_extraction'] += 1
                    
            except Exception as e:
                error_msg = f"å¤„ç†é“¾æ¥å¤±è´¥: {link} - {str(e)}"
                logger.warning(f"âš ï¸ {error_msg}")
                result['statistics']['error_messages'].append(error_msg)
                
                result['failed_articles'].append({
                    'site': site_url,
                    'url': link,
                    'title': _extract_title_from_url(link),
                    'error': str(e),
                    'date': None
                })
                result['statistics']['failed_extraction'] += 1
        
    except Exception as proxy_error:
        proxy_error_msg = str(proxy_error)
        logger.warning(f"âš ï¸ ä»£ç†è®¿é—®å¤±è´¥: {proxy_error_msg}")
        
        if 'ProxyError' in proxy_error_msg or 'SSL' in proxy_error_msg or 'proxy' in proxy_error_msg.lower():
            logger.info(f"ğŸ”„ å°è¯•ç›´è¿è®¿é—®ï¼ˆç»•è¿‡ä»£ç†ï¼‰: {site_url}")
            
            try:
                session.close()
                session = requests.Session()
                
                clear_all_proxy()
                session.trust_env = False
                session.headers.update(BROWSER_HEADERS)
                
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                
                response = session.get(site_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                article_links = _extract_article_links(soup, site_url)
                result['statistics']['total_entries'] = len(article_links)
                
                for link in article_links:
                    try:
                        content_text = _extract_content_with_session_direct(link, session)
                        
                        if content_text:
                            result['articles'].append({
                                'site': site_url,
                                'url': link,
                                'title': _extract_title_from_url(link),
                                'content_text': content_text,
                                'date': None
                            })
                            result['statistics']['successful_extraction'] += 1
                        else:
                            result['failed_articles'].append({
                                'site': site_url,
                                'url': link,
                                'title': _extract_title_from_url(link),
                                'error': 'å†…å®¹æå–å¤±è´¥',
                                'date': None
                            })
                            result['statistics']['failed_extraction'] += 1
                            
                    except Exception as e:
                        error_msg = f"å¤„ç†é“¾æ¥å¤±è´¥: {link} - {str(e)}"
                        logger.warning(f"âš ï¸ {error_msg}")
                        result['statistics']['error_messages'].append(error_msg)
                        
                        result['failed_articles'].append({
                            'site': site_url,
                            'url': link,
                            'title': _extract_title_from_url(link),
                            'error': str(e),
                            'date': None
                        })
                        result['statistics']['failed_extraction'] += 1
                
                logger.info(f"âœ… ç›´è¿è®¿é—®æˆåŠŸ: {site_url}")
                
            except Exception as direct_error:
                error_msg = f"HTMLæŠ“å–å¤±è´¥ï¼ˆä»£ç†å’Œç›´è¿å‡å¤±è´¥ï¼‰ {site_url}: ä»£ç†é”™è¯¯={proxy_error_msg}, ç›´è¿é”™è¯¯={str(direct_error)}"
                logger.error(f"âŒ {error_msg}")
                result['statistics']['error_messages'].append(error_msg)
        else:
            error_msg = f"HTMLæŠ“å–å¤±è´¥ {site_url}: {proxy_error_msg}"
            logger.error(f"âŒ {error_msg}")
            result['statistics']['error_messages'].append(error_msg)
    
    finally:
        session.close()
        os.environ.clear()
        os.environ.update(original_env_backup)
    
    return result


def _extract_content_with_session_direct(url: str, session: requests.Session) -> Optional[str]:
    """ç›´è¿æ–¹å¼æå–å†…å®¹ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰"""
    cached_content = load_html_from_cache(url)
    if cached_content:
        return cached_content
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        response = session.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for script in soup(["script", "style"]):
                script.decompose()
            
            content = soup.get_text()
            if content and len(content.strip()) > 100:
                content = content.strip()
                save_html_to_cache(url, content)
                return content
                
    except Exception as e:
        logger.warning(f"ç›´è¿å†…å®¹æå–å¤±è´¥: {url} - {str(e)}")
    
    return None


def _extract_content_with_session(url: str, session: requests.Session) -> Optional[str]:
    """ä½¿ç”¨æŒ‡å®šä¼šè¯æå–å†…å®¹ï¼Œç”¨äºæ”¯æŒVPNä»£ç†"""
    cached_content = load_html_from_cache(url)
    if cached_content:
        return cached_content
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        response = session.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for script in soup(["script", "style"]):
                script.decompose()
            
            content = soup.get_text()
            if content and len(content.strip()) > 100:
                content = content.strip()
                save_html_to_cache(url, content)
                return content
                
    except Exception as e:
        logger.warning(f"åå¤‡å†…å®¹æå–å¤±è´¥: {url} - {str(e)}")
    
    return None


def _extract_article_links(soup: BeautifulSoup, base_url: str) -> List[str]:
    links = []
    
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        
        if href.startswith('/'):
            href = base_url.rstrip('/') + href
        elif not href.startswith('http'):
            continue
        
        if _is_article_link(href):
            links.append(href)
    
    return list(set(links))


def _is_article_link(url: str) -> bool:
    exclude_patterns = ['#', '/tag/', '/category/', '/author/', '/page/', 'login', 'register']
    
    for pattern in exclude_patterns:
        if pattern in url:
            return False
    
    return True


def _extract_content(url: str) -> Optional[str]:
    """
    æå–æ–‡ç« å†…å®¹ï¼ˆç®€åŒ–ä¸ºåå¤‡æ–¹æ¡ˆï¼‰
    æ³¨æ„ï¼šç°åœ¨RSSå·²æä¾›å®Œæ•´å†…å®¹ï¼Œæ­¤å‡½æ•°ä»…åœ¨RSSå†…å®¹è´¨é‡æå·®æ—¶ä½¿ç”¨
    """
    try:
        # è·å–é…ç½®çš„ä¼šè¯
        session = globals().get('_scraping_session', None)
        
        # ç®€åŒ–è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        if session:
            response = session.get(url, headers=headers, timeout=15)
        else:
            import requests
            response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            content = soup.get_text()
            if content and len(content.strip()) > 100:
                return content.strip()
                
    except Exception as e:
        logger.warning(f"åå¤‡å†…å®¹æå–å¤±è´¥: {url} - {str(e)}")
    
    return None


def _extract_title_from_url(url: str) -> str:
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
    except Exception as e:
        pass
    
    return url


def _is_english(text: str) -> bool:
    if not text:
        return False
    
    english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
    total_chars = sum(1 for char in text if char.isalpha())
    
    if total_chars == 0:
        return False
    
    return english_chars / total_chars > 0.5



def process_articles_with_ai(articles_list: List[Dict], 
                              config: NewsConfig = None,
                              max_workers: int = 10, 
                              batch_size: int = 3) -> str:
    """
    æ‰¹é‡å¹¶è¡Œå¤„ç†æ–‡ç« å¹¶ç”ŸæˆMarkdownï¼ˆæ”¯æŒç¼“å­˜å’Œè‡ªå®šä¹‰é…ç½®ï¼‰
    
    Args:
        articles_list: æ–‡ç« åˆ—è¡¨
        config: NewsConfig é…ç½®å¯¹è±¡ï¼ˆåŒ…å« AI prompt ç­‰é…ç½®ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        batch_size: æ¯ä¸ªæ‰¹é‡å¤„ç†çš„æ–‡ç« æ•°é‡ï¼ˆå»ºè®®3-5ç¯‡ï¼‰
    
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ±‡æ€»æ–‡æœ¬
    """
    if not articles_list:
        return ''
    
    cached_articles = []
    articles_to_process = []
    
    for article in articles_list:
        url = article.get('url', '')
        cached_result = load_ai_from_cache(url)
        if cached_result:
            # logger.info(f"ğŸš€ AIç¼“å­˜å‘½ä¸­: {url}")
            cached_articles.append(cached_result)
        else:
            articles_to_process.append(article)
    
    logger.info(f"ğŸ“Š AIç¼“å­˜ç»Ÿè®¡: {len(cached_articles)}ç¯‡å‘½ä¸­ç¼“å­˜, {len(articles_to_process)}ç¯‡éœ€è¦AIå¤„ç†")
    
    if not articles_to_process:
        logger.info("âœ… æ‰€æœ‰æ–‡ç« å‡å‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡AIå¤„ç†")
        return _generate_markdown(cached_articles, config)
    
    try:
        client = _get_openai_client()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        if cached_articles:
            return _generate_markdown(cached_articles, config)
        return ''
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¹¶è¡ŒAIå¤„ç†: {len(articles_to_process)} ç¯‡æ–‡ç« ")
    logger.info(f"ğŸ¤– AIæ‰¹é‡è®¾ç½®: batch_size={batch_size}, max_workers={max_workers}")
    
    batches = [articles_to_process[i:i + batch_size] for i in range(0, len(articles_to_process), batch_size)]
    logger.info(f"ğŸ¤– åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    newly_processed_articles = []
    completed = 0
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        batch_index_map = {}
        for i, batch in enumerate(batches, 1):
            future = executor.submit(_process_batch_with_ai, client, batch, i, config)
            futures.append(future)
            batch_index_map[id(future)] = i
        
        for future in as_completed(futures):
            try:
                batch_results = future.result()
                newly_processed_articles.extend(batch_results)
                completed += 1
                current_batch_index = batch_index_map.get(id(future), completed)
                logger.info(f"âœ… å®Œæˆ {completed}/{len(batches)} ä¸ªæ‰¹æ¬¡ (æ‰¹æ¬¡ {current_batch_index})")
            except Exception as e:
                current_batch_index = batch_index_map.get(id(future), "æœªçŸ¥")
                logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: æ‰¹æ¬¡ {current_batch_index} - {str(e)}")
                completed += 1
    
    all_processed_articles = cached_articles + newly_processed_articles
    logger.info(f"ğŸ‰ æ‰¹é‡å¹¶è¡ŒAIå¤„ç†å®Œæˆ: {len(cached_articles)}ç¯‡æ¥è‡ªç¼“å­˜, {len(newly_processed_articles)}ç¯‡æ–°å¤„ç†, æ€»è®¡{len(all_processed_articles)}ç¯‡")
    
    markdown_text = _generate_markdown(all_processed_articles, config)
    
    return markdown_text


def _process_batch_with_ai(client: OpenAI, batch: List[Dict], batch_index: int, 
                           config: NewsConfig = None) -> List[Dict]:
    """
    æ‰¹é‡å¤„ç†æ–‡ç« ï¼ˆä¸€æ¬¡å¤„ç†å¤šç¯‡æ–‡ç« ï¼Œæ”¯æŒç¼“å­˜å’Œè‡ªå®šä¹‰é…ç½®ï¼‰
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        batch: æ–‡ç« æ‰¹æ¬¡
        batch_index: æ‰¹æ¬¡ç´¢å¼•
        config: NewsConfig é…ç½®å¯¹è±¡ï¼ˆåŒ…å«è‡ªå®šä¹‰ AI promptï¼‰
    
    Returns:
        å¤„ç†åçš„æ–‡ç« åˆ—è¡¨
    """
    if not batch:
        return []
    
    logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_index}: {len(batch)} ç¯‡æ–‡ç« ")
    
    cached_results = []
    articles_to_process = []
    
    for article in batch:
        url = article.get('url', '')
        cached_result = load_ai_from_cache(url)
        if cached_result:
            logger.info(f"ğŸš€ AIç¼“å­˜å‘½ä¸­: {url}")
            cached_results.append((article, cached_result))
        else:
            articles_to_process.append(article)
    
    if not articles_to_process:
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} å…¨éƒ¨å‘½ä¸­ç¼“å­˜")
        return [result for _, result in cached_results]
    
    if len(articles_to_process) < len(batch):
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_index}: {len(cached_results)}ç¯‡å‘½ä¸­ç¼“å­˜, {len(articles_to_process)}ç¯‡éœ€è¦AIå¤„ç†")
    
    articles_info = []
    for i, article in enumerate(articles_to_process, 1):
        title = article.get('title', '')
        content_text = article.get('content_text', '')
        url = article.get('url', '')
        date_str = article.get('date', '')
        
        articles_info.append(f"""
æ–‡ç«  {i}:
æ ‡é¢˜: {title}
é“¾æ¥: {url}
æ—¥æœŸ: {date_str}
æ­£æ–‡: {content_text[:1500]}...
""")
    
    batch_content = '\n'.join(articles_info)
    
    if not config or not config.ai_prompt:
        raise ValueError("NewsConfig.ai_prompt is required for AI processing")
    
    if not config.ai_system_prompt:
        raise ValueError("NewsConfig.ai_system_prompt is required for AI processing")
    
    ai_prompt_template = config.ai_prompt
    ai_system_prompt = config.ai_system_prompt
    
    prompt = ai_prompt_template.format(
        article_count=len(articles_to_process),
        batch_content=batch_content
    )
    
    try:
        model_name = os.getenv('LLM_MODEL')
        if not model_name:
            raise ValueError('LLM_MODEL environment variable is not set')

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {'role': 'system', 'content': ai_system_prompt},
                {'role': 'user', 'content': prompt}
            ],
            temperature=0.3,
            response_format={'type': 'json_object'},
            timeout=60
        )
        
        result_text = response.choices[0].message.content.strip()
        
        logger.info(f"ğŸ¤– AIè¿”å›åŸå§‹å†…å®¹é•¿åº¦: {len(result_text)} å­—ç¬¦")
        logger.info(f"ğŸ¤– AIè¿”å›åŸå§‹å†…å®¹: {result_text}")
        
        # æ¸…ç† Markdown ä»£ç å—æ ‡è®°ï¼ˆAI æœ‰æ—¶ä¼šè¿”å› ```json ... ```ï¼‰
        if result_text.startswith('```'):
            lines = result_text.split('\n')
            result_text = '\n'.join(lines[1:-1]) if len(lines) > 2 else result_text
            result_text = result_text.strip()
        
        # è§£æJSONç»“æœ
        import json
        results = json.loads(result_text)
        
        logger.info(f"ğŸ” è§£æåçš„resultsç±»å‹: {type(results)}")
        logger.info(f"ğŸ” è§£æåçš„resultså†…å®¹: {str(results)[:1000]}...")
        
        # ç¡®ä¿ç»“æœæ˜¯æ•°ç»„æ ¼å¼
        if not isinstance(results, list):
            logger.warning(f"æ‰¹æ¬¡ {batch_index} è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œå°è¯•ä¿®å¤...")
            results = [results] if not isinstance(results, list) else results
        
        logger.info(f"ğŸ” å¾—åˆ° {len(results)} ç¯‡æ–‡ç« ")
        
        # å°†ç»“æœæ˜ å°„å›åŸå§‹æ–‡ç« æ•°æ®
        # å¤„ç†AIè¿”å›çš„ç»“æœ
        newly_processed = []
        for i, result in enumerate(results):
            if i < len(articles_to_process):  # ç¡®ä¿ä¸è¶…è¿‡éœ€è¦å¤„ç†çš„æ–‡ç« æ•°é‡
                article = articles_to_process[i]
                
                # ç¡®ä¿resultæ˜¯å­—å…¸æ ¼å¼
                if not isinstance(result, dict):
                    logger.warning(f"æ‰¹æ¬¡ {batch_index} æ–‡ç«  {i+1} ç»“æœæ ¼å¼å¼‚å¸¸: {type(result)}")
                    result = {'chinese_title': article.get('title', '')}
                
                processed_article = {
                    'original_title': article.get('title', ''),
                    'chinese_title': result.get('chinese_title', article.get('title', '')),
                    'summary': result.get('summary', article.get('content_text', '')[:200] + '...'),
                    'key_persons': result.get('key_persons', []),
                    'key_person_bios': result.get('key_person_bios', []),
                    'location_name': result.get('location_name', 'æœªçŸ¥åœ°ç‚¹'),
                    'location_context': result.get('location_context', ''),
                    'event_date': result.get('event_date', article.get('date', '')),
                    'curated_angles': result.get('curated_angles', []),
                    'url': article.get('url', ''),
                    'date': article.get('date', ''),
                    'site': article.get('site', ''),
                    'ai_processed_at': datetime.now().isoformat(),
                    'content_length': len(article.get('content_text', ''))
                }
                newly_processed.append(processed_article)
        
        # åˆå¹¶ç¼“å­˜ç»“æœå’Œæ–°å¤„ç†çš„ç»“æœ
        all_processed = [result for _, result in cached_results] + newly_processed
        
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} å¤„ç†æˆåŠŸ: {len(cached_results)}ç¯‡æ¥è‡ªç¼“å­˜, {len(newly_processed)}ç¯‡æ–°å¤„ç†")
        
        # åªä¸ºæ–°å¤„ç†çš„æ–‡ç« ä¿å­˜AIç¼“å­˜
        for processed_article in newly_processed:
            url = processed_article.get('url', '')
            if url:
                save_ai_to_cache(url, processed_article)
        
        return all_processed
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ {batch_index} AIå¤„ç†å¤±è´¥: {str(e)}")
        
        # å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
        processed = []
        for article in batch:
            processed.append({
                'original_title': article.get('title', ''),
                'chinese_title': article.get('title', ''),
                'summary': article.get('content_text', '')[:200] + '...',
                'key_persons': [],
                'key_person_bios': [],
                'location_name': 'æœªçŸ¥åœ°ç‚¹',
                'location_context': '',
                'event_date': article.get('date', ''),
                'curated_angles': [],
                'url': article.get('url', ''),
                'date': article.get('date', ''),
                'site': article.get('site', ''),
                'ai_processed_at': datetime.now().isoformat(),
                'content_length': len(article.get('content_text', '')),
                'error': str(e)
            })
        
        # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†
        for processed_article in processed:
            url = processed_article.get('url', '')
            if url:
                save_ai_to_cache(url, processed_article)
        
        return processed


def _generate_markdown(articles: List[Dict], config: NewsConfig = None) -> str:
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æ–°é—»æ±‡æ€»å†…å®¹
    
    Args:
        articles: å¤„ç†åçš„æ–‡ç« åˆ—è¡¨
        config: NewsConfig é…ç½®å¯¹è±¡ï¼ˆåŒ…å«è‡ªå®šä¹‰æ ‡é¢˜ç­‰ï¼‰
    
    Returns:
        Markdown æ ¼å¼çš„æ–°é—»æ±‡æ€»æ–‡æœ¬
    """
    if not articles:
        return ''
    
    markdown_lines = []
    
    report_header = config.report_header if config and config.report_header else '# æˆ·å¤–è¿åŠ¨æ–°é—»æ±‡æ€»\n'
    markdown_lines.append(report_header)
    markdown_lines.append(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
    markdown_lines.append(f'å…±æ”¶å½• {len(articles)} ç¯‡æ–‡ç« \n')
    
    source_sites = list(set(article.get('site') for article in articles if isinstance(article, dict) and article.get('site')))
    if source_sites:
        markdown_lines.append('\n## æœç´¢æ¥æºç½‘ç«™\n')
        for site in source_sites:
            markdown_lines.append(f'- {site}\n')
        markdown_lines.append('\n---\n')
    
    for i, article in enumerate(articles, 1):
        # ç¡®ä¿ article æ˜¯å­—å…¸ç±»å‹
        if not isinstance(article, dict):
            logger.warning(f"âš ï¸ è·³è¿‡éå­—å…¸ç±»å‹çš„æ–‡ç« : {type(article)}")
            continue
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        chinese_title = article.get('chinese_title', 'æœªçŸ¥æ ‡é¢˜')
        markdown_lines.append(f'\n## {i}. {chinese_title}\n')
        
        if article.get('original_title') and article.get('original_title') != article.get('chinese_title'):
            markdown_lines.append(f'**åŸæ ‡é¢˜**: {article["original_title"]}\n')
        
        if article.get('date'):
            markdown_lines.append(f'**æ—¥æœŸ**: {article["date"]}\n')
        
        # å¤„ç†é“¾æ¥å­—æ®µ
        url = article.get('url', 'æœªçŸ¥é“¾æ¥')
        markdown_lines.append(f'**é“¾æ¥**: {url}\n')
        
        if article.get('event_date'):
            markdown_lines.append(f'**äº‹ä»¶æ—¥æœŸ**: {article["event_date"]}\n')
        
        if article.get('location_name'):
            location_name = article["location_name"]
            location_context = article.get("location_context", "")
            if location_context:
                markdown_lines.append(f'**åœ°ç‚¹**: {location_name}ã€‚{location_context}\n')
            else:
                markdown_lines.append(f'**åœ°ç‚¹**: {location_name}\n')
        else:
            markdown_lines.append(f'**åœ°ç‚¹**: æ— \n')
        
        if article.get('key_persons'):
            markdown_lines.append(f'**å…³é”®äººç‰©**:\n')
            key_persons = article['key_persons']
            key_person_bios = article.get('key_person_bios', [])
            for j, name in enumerate(key_persons):
                person_encoded = name.replace(' ', '+')
                search_url = f"https://www.google.com/search?q={person_encoded}+outdoor"
                bio = key_person_bios[j] if j < len(key_person_bios) else ''
                if bio:
                    markdown_lines.append(f'- [{name}]({search_url})ï¼š{bio}\n')
                else:
                    markdown_lines.append(f'- [{name}]({search_url})\n')
            markdown_lines.append('\n')
        else:
            markdown_lines.append(f'**å…³é”®äººç‰©**: æ— \n')
        
        if article.get('curated_angles'):
            angles = article['curated_angles']
            markdown_lines.append(f'**é€‰é¢˜æ¨è**:\n')
            for angle_item in angles:
                markdown_lines.append(f'  - {angle_item}\n')
        else:
            markdown_lines.append(f'**é€‰é¢˜æ¨è**: æ— \n')
        
        markdown_lines.append(f'\n**æ‘˜è¦**: {article["summary"]}\n')
        
        markdown_lines.append('\n---\n')
    
    return ''.join(markdown_lines)


def _parse_text_with_links(text):
    """
    [å†…éƒ¨å·¥å…·] è§£æåŒ…å« Markdown é“¾æ¥çš„æ–‡æœ¬
    è¾“å…¥: "ç‚¹å‡» [è¿™é‡Œ](http://google.com) æŸ¥çœ‹"
    è¾“å‡º: é£ä¹¦ TextElement ç»“æ„æ•°ç»„
    """
    elements = []
    # æ­£åˆ™åŒ¹é… [text](url)
    pattern = re.compile(r'\[(.*?)\]\((.*?)\)')
    last_idx = 0
    
    for match in pattern.finditer(text):
        # 1. æ·»åŠ é“¾æ¥å‰çš„æ™®é€šæ–‡æœ¬
        if match.start() > last_idx:
            elements.append(TextElement(
                text_run=TextRun(content=text[last_idx:match.start()])
            ))
        
        # 2. æ·»åŠ é“¾æ¥æ–‡æœ¬
        link_text = match.group(1)
        link_url = match.group(2)
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(link_text)
                .text_element_style(TextElementStyle.builder()
                    .link(Link.builder().url(link_url).build())
                    .build())
                .build())
            .build())
        last_idx = match.end()
    
    # 3. æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
    if last_idx < len(text):
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text[last_idx:])
                .build())
            .build())
        
    # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥è¿”å›çº¯æ–‡æœ¬
    if not elements:
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text)
                .build())
            .build())
        
    return elements

def publish_feishu_report(report_title, markdown_content, chat_id, 
                          collaborator_openids: List[str] = None):
    """
    å‘å¸ƒæ–°é—»æ±‡æ€»åˆ°é£ä¹¦æ–‡æ¡£
    
    æ ¸å¿ƒåŠŸèƒ½: åˆ›å»ºæ–‡æ¡£ -> å†™å…¥å†…å®¹ -> å‘é€å¡ç‰‡
    
    Args:
        report_title: æ–°é—»æ±‡æ€»æ ‡é¢˜
        markdown_content: Markdown æ ¼å¼çš„æ–°é—»æ±‡æ€»å†…å®¹
        chat_id: é£ä¹¦ç¾¤ç»„ ID
        collaborator_openids: åä½œè€… openid åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆäºç¯å¢ƒå˜é‡ï¼‰
    
    Returns:
        é£ä¹¦æ–‡æ¡£é“¾æ¥ï¼Œå¤±è´¥è¿”å› None
    """
    print(f"ğŸš€ [Feishu] å‡†å¤‡å‘å¸ƒæ–‡æ¡£: {report_title}")
    
    # è·å–é£ä¹¦å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¸…é™¤ä»£ç†ï¼‰
    client = get_feishu_client()
    
    # =================================================
    # æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºç™½æ–‡æ¡£
    # =================================================
    try:
        create_req = CreateDocumentRequest.builder() \
            .request_body(CreateDocumentRequestBody.builder()
                .title(report_title)
                .build()) \
            .build()
            
        resp = client.docx.v1.document.create(create_req)
        
        if not resp.success():
            print(f"âŒ åˆ›å»ºæ–‡æ¡£å¤±è´¥: {resp.code} - {resp.msg}")
            return None
            
        document_id = resp.data.document.document_id
        # æ³¨æ„: åªæœ‰é£ä¹¦å›½å†…ç‰ˆæ˜¯ feishu.cnï¼Œå›½é™…ç‰ˆè¯·æ”¹ä¸º larksuite.com
        doc_url = f"https://feishu.cn/docx/{document_id}"
        print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")

        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„åä½œè€…åˆ—è¡¨ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡
        openids = collaborator_openids if collaborator_openids else []
        if not openids:
            env_openids = os.getenv("FEISHU_COLLABORATOR_OPENIDS", "")
            if env_openids:
                openids = [oid.strip() for oid in env_openids.split(",") if oid.strip()]
        
        collaborator_perm = os.getenv("FEISHU_COLLABORATOR_PERM", "edit")
        
        if openids:
            
            added_count = 0
            failed_count = 0
            
            for openid in openids:
                try:
                    add_req = CreatePermissionMemberRequest.builder() \
                        .token(document_id) \
                        .type("docx") \
                        .need_notification(False) \
                        .request_body(BaseMember.builder()
                            .member_type("openid")
                            .member_id(openid)
                            .perm(collaborator_perm)
                            .perm_type("container")
                            .type("user")
                            .build()) \
                        .build()
                    
                    add_resp = client.drive.v1.permission_member.create(add_req)
                    
                    if add_resp.success():
                        print(f"âœ… åä½œè€…æ·»åŠ æˆåŠŸ: {openid}")
                        added_count += 1
                    else:
                        print(f"âš ï¸ åä½œè€…æ·»åŠ å¤±è´¥: {openid} - {add_resp.msg}")
                        failed_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ ä¸º {openid} æ·»åŠ åä½œè€…æ—¶å‡ºé”™: {e}")
                    failed_count += 1
            
            if added_count > 0:
                print(f"âœ… æˆåŠŸæ·»åŠ  {added_count} ä¸ªåä½œè€…ï¼Œæƒé™: {collaborator_perm}")
            if failed_count > 0:
                print(f"âš ï¸ {failed_count} ä¸ªåä½œè€…æ·»åŠ å¤±è´¥")

    except Exception as e:
        print(f"âŒ é£ä¹¦ API è¿æ¥é”™è¯¯: {e}")
        return None

    # =================================================
    # æ­¥éª¤ 2: ä½¿ç”¨é£ä¹¦å®˜æ–¹ API å°† Markdown è½¬æ¢ä¸º Blocks
    # =================================================
    print("ğŸ”„ æ­£åœ¨å°† Markdown è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£å—...")
    
    # è°ƒç”¨é£ä¹¦å®˜æ–¹çš„ Markdown è½¬æ¢ API
    convert_req = ConvertDocumentRequest.builder() \
        .request_body(ConvertDocumentRequestBody.builder()
            .content_type("markdown")
            .content(markdown_content)
            .build()) \
        .build()
    
    convert_resp = client.docx.v1.document.convert(convert_req)
    
    if not convert_resp.success():
        print(f"âŒ Markdown è½¬æ¢å¤±è´¥: {convert_resp.code} - {convert_resp.msg}")
        return None
    
    # è·å–è½¬æ¢åçš„ blocks
    blocks = convert_resp.data.blocks
    first_level_block_ids = convert_resp.data.first_level_block_ids or []
    
    if not blocks:
        print("âš ï¸ è½¬æ¢åçš„å†…å®¹ä¸ºç©º")
        return doc_url
    
    # ä½¿ç”¨ first_level_block_ids é‡æ–°æ’åº blocks
    if first_level_block_ids:
        block_map = {b.block_id: b for b in blocks}
        ordered_blocks = []
        for block_id in first_level_block_ids:
            if block_id in block_map:
                ordered_blocks.append(block_map[block_id])
        # æ·»åŠ ä¸åœ¨ first_level_block_ids ä¸­çš„ blocks
        for block in blocks:
            if block.block_id not in first_level_block_ids:
                ordered_blocks.append(block)
        blocks = ordered_blocks
    
    print(f"âœ… Markdown è½¬æ¢æˆåŠŸï¼Œå…± {len(blocks)} ä¸ª blocks")
    
    # =================================================
    # æ­¥éª¤ 3: ä½¿ç”¨è½¬æ¢å¥½çš„ blocks å†™å…¥æ–‡æ¡£å†…å®¹
    # =================================================
    print("ğŸ“ æ­£åœ¨å†™å…¥æ–‡æ¡£å†…å®¹...")
    
    try:
        # å°† blocks åˆ†æ‰¹å†™å…¥ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
        batch_size = 50
        for i in range(0, len(blocks), batch_size):
            batch = blocks[i:i + batch_size]
            
            # ç›´æ¥ä½¿ç”¨è½¬æ¢å¥½çš„ block å¯¹è±¡
            batch_req = CreateDocumentBlockChildrenRequest.builder() \
                .document_id(document_id) \
                .block_id(document_id) \
                .request_body(CreateDocumentBlockChildrenRequestBody.builder()
                    .children(batch)
                    .build()) \
                .build()
            
            batch_resp = client.docx.v1.document_block_children.create(batch_req)
            
            if not batch_resp.success():
                print(f"âš ï¸ æ‰¹æ¬¡å†™å…¥å¤±è´¥ (æ‰¹æ¬¡ {i//batch_size + 1}): {batch_resp.code} - {batch_resp.msg}")
            else:
                print(f"âœ… æ‰¹æ¬¡å†™å…¥æˆåŠŸ (æ‰¹æ¬¡ {i//batch_size + 1}): {len(batch)} ä¸ª blocks")
        
        print(f"âœ… æ–‡æ¡£å†…å®¹å†™å…¥å®Œæˆï¼Œå…± {len(blocks)} ä¸ª blocks")
            
    except Exception as e:
        print(f"âš ï¸ å†™å…¥æ–‡æ¡£å†…å®¹æ—¶å‡ºé”™: {e}")
        print("ğŸ“ è·³è¿‡å†…å®¹å†™å…¥ï¼Œç»§ç»­å‘é€é€šçŸ¥...")
        # å³ä½¿å‡ºé”™ï¼Œä¹Ÿç»§ç»­åç»­æ­¥éª¤

    # =================================================
    # æ­¥éª¤ 4: å‘é€å¯Œæ–‡æœ¬å¡ç‰‡æ¶ˆæ¯
    # =================================================
    print(f"ğŸ“¤ æ­£åœ¨æ¨é€åˆ°ç¾¤ç»„: {chat_id}")
    
    # æ„é€ å¡ç‰‡ JSON
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": "ğŸ§—â€â™‚ï¸ æˆ·å¤–èµ„è®¯æ–°é—»æ±‡æ€»å·²ç”Ÿæˆ"},
            "template": "blue"
        },
        "elements": [
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"æœ¬æœŸèµ„è®¯å·²ç”± AI æ•´ç†å®Œæ¯•ã€‚\n**æ ‡é¢˜ï¼š** {report_title}\n**æ—¶é—´ï¼š** {os.getenv('TODAY', 'æœ¬æœŸ')}"
                }
            },
            {
                "tag": "hr"
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ‘‰ ç‚¹å‡»é˜…è¯»å®Œæ•´æ–°é—»æ±‡æ€»"},
                        "url": doc_url,
                        "type": "primary"
                    }
                ]
            }
        ]
    }

    # å‘é€è¯·æ±‚
    msg_req = CreateMessageRequest.builder() \
        .receive_id_type("chat_id") \
        .request_body(CreateMessageRequestBody.builder() \
            .receive_id(chat_id) \
            .msg_type("interactive") \
            .content(json.dumps(card_content)) \
            .build()) \
        .build()

    try:
        msg_resp = client.im.v1.message.create(msg_req)
        
        if msg_resp.success():
            print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
        else:
            print(f"âš ï¸ æ¶ˆæ¯æ¨é€å¤±è´¥: {msg_resp.code} - {msg_resp.msg}")
            print("ğŸ“ ä»ç„¶è¿”å›æ–‡æ¡£URL...")
    except Exception as e:
        print(f"âš ï¸ å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
        print("ğŸ“ ä»ç„¶è¿”å›æ–‡æ¡£URL...")
    
    # å…³é”®ï¼šå§‹ç»ˆè¿”å›æ–‡æ¡£URLï¼Œå³ä½¿å†…å®¹å†™å…¥æˆ–æ¶ˆæ¯æ¨é€å¤±è´¥
    print(f"ğŸ‰ é£ä¹¦æ–‡æ¡£å‘å¸ƒå®Œæˆ!")
    print(f"ğŸ“„ æ–‡æ¡£é“¾æ¥: {doc_url}")
    return doc_url
