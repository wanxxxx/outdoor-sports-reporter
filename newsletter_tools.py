import os
import json
import re
import hashlib
import pickle
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

import feedparser
import requests
from bs4 import BeautifulSoup
import trafilatura
from openai import OpenAI
from dotenv import load_dotenv

import lark_oapi as lark
from lark_oapi.api.docx.v1 import (
    CreateDocumentRequest,
    CreateDocumentRequestBody,
    ConvertDocumentRequest,
    ConvertDocumentRequestBody,
    TextElement,
    TextRun,
    TextElementStyle,
    Link,
    UpdateBlockRequest,
    BatchUpdateDocumentBlockRequest,
    BatchUpdateDocumentBlockRequestBody,
    CreateDocumentBlockChildrenRequest,
    CreateDocumentBlockChildrenRequestBody,
    Block,
    Text as TextModel
)
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody
from lark_oapi.api.drive.v1 import (
    CreatePermissionMemberRequest,
    BaseMember,
    BatchCreatePermissionMemberRequest,
    BatchCreatePermissionMemberRequestBody
)

load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä¿å­˜åŸå§‹ä»£ç†è®¾ç½®
_original_proxy_settings = {
    'HTTP_PROXY': os.environ.get('HTTP_PROXY'),
    'HTTPS_PROXY': os.environ.get('HTTPS_PROXY'),
    'ALL_PROXY': os.environ.get('ALL_PROXY')
}

# RSSç¼“å­˜é…ç½®
RSS_CACHE_DIR = "cache/rss"
RSS_CACHE_TTL = 3600  # 1å°æ—¶ç¼“å­˜

# AIå¤„ç†ç¼“å­˜é…ç½®
AI_CACHE_DIR = "cache/ai"
AI_CACHE_TTL = 86400 * 7  # 7å¤©ç¼“å­˜ï¼ˆAIå¤„ç†ç»“æœé•¿æœŸæœ‰æ•ˆï¼‰

# åˆ›å»ºç¼“å­˜ç›®å½•
os.makedirs(RSS_CACHE_DIR, exist_ok=True)
os.makedirs(AI_CACHE_DIR, exist_ok=True)


def clean_expired_cache(cache_dir: str, ttl: int, cache_type: str = "cache"):
    """
    æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        ttl: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
        cache_type: ç¼“å­˜ç±»å‹ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    if not os.path.exists(cache_dir):
        return
    
    current_time = datetime.now().timestamp()
    cleaned_count = 0
    total_size = 0
    
    try:
        for filename in os.listdir(cache_dir):
            filepath = os.path.join(cache_dir, filename)
            
            if os.path.isfile(filepath):
                file_time = os.path.getmtime(filepath)
                file_age = current_time - file_time
                
                if file_age > ttl:
                    file_size = os.path.getsize(filepath)
                    os.remove(filepath)
                    cleaned_count += 1
                    total_size += file_size
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸ{cache_type}: {filename} (å·²è¿‡æœŸ {file_age // 3600:.1f} å°æ—¶)")
        
        if cleaned_count > 0:
            size_mb = total_size / (1024 * 1024)
            logger.info(f"âœ… {cache_type}æ¸…ç†å®Œæˆ: åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶, é‡Šæ”¾ {size_mb:.2f} MB")
        else:
            logger.info(f"âœ… {cache_type}æ— éœ€æ¸…ç†: æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æœ‰æ•ˆæœŸå†…")
            
    except Exception as e:
        logger.warning(f"âš ï¸ æ¸…ç†{cache_type}å¤±è´¥: {str(e)}")


def clean_all_expired_caches():
    """æ¸…ç†æ‰€æœ‰è¿‡æœŸçš„ç¼“å­˜"""
    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜...")
    
    clean_expired_cache(RSS_CACHE_DIR, RSS_CACHE_TTL, "RSSç¼“å­˜")
    clean_expired_cache(AI_CACHE_DIR, AI_CACHE_TTL, "AIç¼“å­˜")
    
    logger.info("ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆ")


def get_rss_cache_path(rss_url: str) -> str:
    """è·å–RSSç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨URLçš„hashä½œä¸ºæ–‡ä»¶å
    url_hash = hashlib.md5(rss_url.encode()).hexdigest()
    return os.path.join(RSS_CACHE_DIR, f"{url_hash}.pkl")


def load_rss_from_cache(rss_url: str) -> Optional[feedparser.FeedParserDict]:
    """ä»ç¼“å­˜åŠ è½½RSSæ•°æ®"""
    cache_path = get_rss_cache_path(rss_url)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
        cache_time = os.path.getmtime(cache_path)
        current_time = datetime.now().timestamp()
        
        if current_time - cache_time > RSS_CACHE_TTL:
            logger.info(f"ğŸ“¦ RSSç¼“å­˜è¿‡æœŸ: {rss_url}")
            return None
        
        with open(cache_path, 'rb') as f:
            cached_data = pickle.load(f)
        logger.info(f"ğŸ“¦ RSSç¼“å­˜å‘½ä¸­: {rss_url}")
        return cached_data
        
    except Exception as e:
        logger.warning(f"ğŸ“¦ RSSç¼“å­˜åŠ è½½å¤±è´¥: {rss_url} - {str(e)}")
        return None


def save_rss_to_cache(rss_url: str, feed_data: feedparser.FeedParserDict) -> bool:
    """ä¿å­˜RSSæ•°æ®åˆ°ç¼“å­˜"""
    cache_path = get_rss_cache_path(rss_url)
    
    try:
        with open(cache_path, 'wb') as f:
            pickle.dump(feed_data, f)
        logger.info(f"ğŸ“¦ RSSç¼“å­˜ä¿å­˜: {rss_url}")
        return True
    except Exception as e:
        logger.warning(f"ğŸ“¦ RSSç¼“å­˜ä¿å­˜å¤±è´¥: {rss_url} - {str(e)}")
        return False


# ================================
# AIå¤„ç†ç¼“å­˜å‡½æ•°
# ================================

def get_ai_cache_path(url: str) -> str:
    """è·å–AIç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨URLçš„hashä½œä¸ºæ–‡ä»¶å
    url_hash = hashlib.md5(url.encode()).hexdigest()
    return os.path.join(AI_CACHE_DIR, f"{url_hash}.json")


def load_ai_from_cache(url: str) -> Optional[Dict]:
    """ä»ç¼“å­˜åŠ è½½AIå¤„ç†ç»“æœ"""
    cache_path = get_ai_cache_path(url)
    
    if not os.path.exists(cache_path):
        return None
    
    try:
        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
        cache_time = os.path.getmtime(cache_path)
        current_time = datetime.now().timestamp()
        
        if current_time - cache_time > AI_CACHE_TTL:
            logger.info(f"ğŸ“¦ AIç¼“å­˜è¿‡æœŸ: {url}")
            return None
        
        with open(cache_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
        logger.info(f"ğŸ“¦ AIç¼“å­˜å‘½ä¸­: {url}")
        return cached_data
        
    except Exception as e:
        logger.warning(f"ğŸ“¦ AIç¼“å­˜åŠ è½½å¤±è´¥: {url} - {str(e)}")
        return None


def save_ai_to_cache(url: str, ai_result: Dict) -> bool:
    """ä¿å­˜AIå¤„ç†ç»“æœåˆ°ç¼“å­˜"""
    cache_path = get_ai_cache_path(url)
    
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(ai_result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“¦ AIç¼“å­˜ä¿å­˜: {url}")
        return True
    except Exception as e:
        logger.warning(f"ğŸ“¦ AIç¼“å­˜ä¿å­˜å¤±è´¥: {url} - {str(e)}")
        return False


def parse_rss_with_cache(rss_url: str) -> Optional[feedparser.FeedParserDict]:
    """è§£æRSSï¼Œæ”¯æŒç¼“å­˜"""
    # å°è¯•ä»ç¼“å­˜åŠ è½½
    feed = load_rss_from_cache(rss_url)
    if feed:
        return feed
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè§£æRSS
    logger.info(f"ğŸ” è§£æRSS: {rss_url}")
    feed = feedparser.parse(rss_url)
    
    if feed.entries:
        # ä¿å­˜åˆ°ç¼“å­˜
        save_rss_to_cache(rss_url, feed)
    
    return feed


def enable_proxy_for_web_scraping():
    """
    æ¢å¤ä»£ç†è®¾ç½®ï¼ˆç”¨äºç½‘ç«™æŠ“å–ï¼‰
    """
    # æ¢å¤ä»£ç†ç¯å¢ƒå˜é‡
    for key, value in _original_proxy_settings.items():
        if value:
            os.environ[key] = value
    logger.info("ğŸŒ æ¢å¤ä»£ç†è®¾ç½® (ç”¨äºç½‘ç«™æŠ“å–)")

def clear_all_proxy():
    """
    å®Œå…¨æ¸…é™¤æ‰€æœ‰ä»£ç†è®¾ç½®
    """
    # æ¸…é™¤æ‰€æœ‰å¯èƒ½çš„ä»£ç†å˜é‡
    proxy_vars = [
        'HTTP_PROXY', 'HTTPS_PROXY', 'ALL_PROXY',
        'http_proxy', 'https_proxy', 'all_proxy',
        'HTTP_PROXY_HOST', 'HTTP_PROXY_PORT',
        'HTTPS_PROXY_HOST', 'HTTPS_PROXY_PORT',
        'NO_PROXY', 'no_proxy'
    ]
    for var in proxy_vars:
        os.environ.pop(var, None)
    
    # ç¦ç”¨å½“å‰è¿›ç¨‹çš„æ‰€æœ‰ç½‘ç»œä»£ç†
    os.environ['NO_PROXY'] = '*'
    os.environ['no_proxy'] = '*'
    
    # æ¸…é™¤requestsçš„ä»£ç†è®¾ç½®
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    session = requests.Session()
    session.trust_env = False
    retry_strategy = Retry(
        total=2,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    logger.info("ğŸš« å®Œå…¨æ¸…é™¤æ‰€æœ‰ä»£ç†è®¾ç½®")


def get_feishu_client():
    """
    è·å–é£ä¹¦å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¸…é™¤ä»£ç†ï¼‰
    """
    clear_all_proxy()
    
    try:
        client = lark.Client.builder() \
            .app_id(os.getenv("FEISHU_APP_ID")) \
            .app_secret(os.getenv("FEISHU_APP_SECRET")) \
            .log_level(lark.LogLevel.INFO) \
            .build()
        
        return client
    except Exception as e:
        logger.error(f"åˆ›å»ºé£ä¹¦å®¢æˆ·ç«¯å¤±è´¥: {e}")
        # å¦‚æœé…ç½®å¤±è´¥ï¼Œåˆ›å»ºæœ€åŸºæœ¬çš„å®¢æˆ·ç«¯
        client = lark.Client.builder() \
            .app_id(os.getenv("FEISHU_APP_ID")) \
            .app_secret(os.getenv("FEISHU_APP_SECRET")) \
            .build()
        return client


def _get_openai_client():
    """
    è·å–OpenAIå®¢æˆ·ç«¯ï¼ˆæ¸…é™¤ä»£ç†ï¼‰
    """
    clear_all_proxy()
    
    api_key = os.getenv('LLM_API_KEY')
    base_url = os.getenv('LLM_BASE_URL')
    
    if not api_key:
        raise ValueError('LLM_API_KEY environment variable is not set')
    
    client_kwargs = {'api_key': api_key}
    if base_url:
        client_kwargs['base_url'] = base_url
    
    return OpenAI(**client_kwargs)

# ä»ç¯å¢ƒå˜é‡è¯»å–ç½‘ç«™åˆ—è¡¨ï¼Œæ ¼å¼ï¼šç”¨é€—å·åˆ†éš”çš„URLåˆ—è¡¨
TARGET_SITES = os.getenv('TARGET_SITES', '').split(',') if os.getenv('TARGET_SITES') else []
TARGET_SITES = [site.strip() for site in TARGET_SITES if site.strip()]

# ä»ç¯å¢ƒå˜é‡è¯»å–RSSæ˜ å°„ï¼Œæ ¼å¼ï¼šsite1_url=rss1_url,site2_url=rss2_url
RSS_FEEDS_ENV = os.getenv('RSS_FEEDS', '')
RSS_FEEDS = {}

if RSS_FEEDS_ENV:
    for mapping in RSS_FEEDS_ENV.split(','):
        if '=' in mapping:
            site_url, rss_url = mapping.split('=', 1)
            RSS_FEEDS[site_url.strip()] = rss_url.strip()

def fetch_outdoor_articles(start_date: date, end_date: date, max_workers: int = 3) -> List[Dict]:
    """
    å¹¶è¡ŒæŠ“å–æˆ·å¤–è¿åŠ¨ç›¸å…³æ–‡ç« 
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ˆä»…ç”¨äºç½‘ç«™çº§å¹¶å‘ï¼‰
    
    Returns:
        æ–‡ç« åˆ—è¡¨
    """
    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡ŒæŠ“å–æ–‡ç« : {start_date} åˆ° {end_date}")
    
    # ç¡®ä¿ç½‘ç«™æŠ“å–æ—¶ä½¿ç”¨ä»£ç†
    enable_proxy_for_web_scraping()
    
    # ç½‘ç«™çº§å¹¶å‘ï¼šå¤šä¸ªRSSæºåŒæ—¶æŠ“å–ï¼Œæ¯ä¸ªç½‘ç«™å†…éƒ¨ä¸²è¡Œå¤„ç†
    # ä¼˜åŒ–åçš„å¹¶å‘ç­–ç•¥ï¼šmax_workers=3 ç¡®ä¿æœ€å¤š3ä¸ªç½‘ç«™åŒæ—¶æŠ“å–
    # æ¯ä¸ªç½‘ç«™å†…éƒ¨çš„æ–‡ç« æå–éƒ½æ˜¯ä¸²è¡Œçš„ï¼Œé¿å…åµŒå¥—å¹¶å‘å’Œè¿æ¥æ± é—®é¢˜
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æŠ“å–ä»»åŠ¡
        futures = []
        site_url_map = {}  # è®°å½•futureå’Œå¯¹åº”ç½‘ç«™URLçš„æ˜ å°„
        for site_url in TARGET_SITES:
            rss_feed = RSS_FEEDS.get(site_url)
            
            if rss_feed:
                future = executor.submit(_fetch_from_rss, rss_feed, site_url, start_date, end_date)
            else:
                future = executor.submit(_fetch_from_html, site_url, start_date, end_date)
            
            futures.append(future)
            site_url_map[id(future)] = site_url  # è®°å½•æ˜ å°„å…³ç³»
        
        # æ”¶é›†ç»“æœ
        articles = []
        completed = 0
        for future in as_completed(futures):
            try:
                site_result = future.result()
                current_site_url = site_url_map.get(id(future), "æœªçŸ¥ç½‘ç«™")
                
                # å¤„ç†ä¸åŒç±»å‹çš„è¿”å›å€¼
                if isinstance(site_result, dict):
                    # _fetch_from_html è¿”å›å­—å…¸
                    site_articles = site_result.get('articles', [])
                    articles.extend(site_articles)
                elif isinstance(site_result, list):
                    # _fetch_from_rss è¿”å›åˆ—è¡¨
                    articles.extend(site_result)
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥è¿”å›ç±»å‹: {type(site_result)}")
                
                completed += 1
                logger.info(f"âœ… å®Œæˆ {completed}/{len(TARGET_SITES)} ä¸ªç½‘ç«™ï¼š{current_site_url}")
            except Exception as e:
                current_site_url = site_url_map.get(id(future), "æœªçŸ¥ç½‘ç«™")
                logger.error(f"âŒ æŠ“å–ç½‘ç«™å¤±è´¥: {current_site_url} - {str(e)}")
                completed += 1
    
    logger.info(f"ğŸ‰ æ‰€æœ‰ç½‘ç«™æŠ“å–å®Œæˆï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def _clean_rss_content(raw_html: str) -> str:
    # 1. å¤„ç†è½¬ä¹‰å­—ç¬¦ï¼šå°† \\n æ›¿æ¢ä¸ºæ¢è¡Œï¼Œå°† \\\" æ›¿æ¢ä¸ºå¼•å·
    content = raw_html.replace('\\n', '\n').replace('\\"', '"')
    
    # 2. ä½¿ç”¨ BeautifulSoup è§£æ HTML
    soup = BeautifulSoup(content, 'html.parser')
    
    # 3. ç§»é™¤ä¸ç›¸å…³çš„æ ‡ç­¾ï¼ˆå¦‚å›¾ç‰‡è¯´æ˜ã€è„šæœ¬ã€æ ·å¼ï¼‰
    for extra in soup(['figure', 'script', 'style', 'img']):
        extra.decompose()
        
    # 4. è·å–æ–‡æœ¬ï¼Œå¹¶å¤„ç†æ‰ RSS ä¸­å¸¸è§çš„é‡å¤é“¾æ¥ï¼ˆæ¯”å¦‚ "The post...appeared first on..."ï¼‰
    lines = []
    for p in soup.find_all(['p', 'h1', 'h2', 'h3']):
        text = p.get_text().strip()
        # è¿‡æ»¤æ‰ RSS è‡ªåŠ¨ç”Ÿæˆçš„æœ«å°¾æ¨å¹¿è¯­
        if "The post" in text and "appeared first on" in text:
            continue
        if text:
            lines.append(text)
    
    # 5. å»é‡ï¼ˆRSS æœ‰æ—¶ä¼šé‡å¤æ¨é€æ­£æ–‡ç‰‡æ®µï¼‰
    unique_lines = []
    for line in lines:
        if line not in unique_lines:
            unique_lines.append(line)
            
    return "\n\n".join(unique_lines)


def _fetch_from_rss(rss_url: str, site_url: str, start_date: date, end_date: date) -> List[Dict]:
    """
    ä»RSSæºæŠ“å–æ–‡ç« ï¼ˆä¸²è¡Œæ‰§è¡Œï¼Œç®€åŒ–é€»è¾‘ï¼‰
    """
    articles = []
    
    try:
        # ä½¿ç”¨ç¼“å­˜è§£æRSS
        feed = parse_rss_with_cache(rss_url)
        if not feed:
            logger.warning(f"RSSè§£æå¤±è´¥: {rss_url}")
            return articles
            
        logger.info(f"ğŸ” RSS[{rss_url}] ä¸­å…±æœ‰ {len(feed.entries)} æ¡ç›®")
        
        # æ­¥éª¤1: è§£æRSSï¼ˆå¿«é€Ÿï¼Œæœ¬åœ°å¤„ç†ï¼‰
        # æ­¥éª¤2: è¿‡æ»¤æ—¥æœŸèŒƒå›´å¹¶ç›´æ¥æå–RSSå†…å®¹ï¼ˆé¿å…ç½‘é¡µæŠ“å–ï¼‰
        article_data = []
        
        for entry in feed.entries:
            if hasattr(entry, 'published_parsed'):
                article_date = datetime(*entry.published_parsed[:6])
                title = entry.get('title', '')
                
                if start_date <= article_date.date() <= end_date:
                    # æ–‡ç« æ—¥æœŸåœ¨èŒƒå›´å†…ï¼Œç›´æ¥ä»RSSæå–å†…å®¹
                    article_url = entry.get('link', '')
                    
                    # ç›´æ¥ä»RSSæ¡ç›®ä¸­æå–å†…å®¹
                    description = entry.get('description', '')
                    summary = entry.get('summary', '')
                    
                    # å°è¯•è·å–å®Œæ•´çš„æ–‡ç« å†…å®¹
                    content_encoded = ''
                    if entry.get('content'):
                        # feedparserä¼šå°†contentå­—æ®µè§£æä¸ºåˆ—è¡¨
                        content_list = entry.get('content', [])
                        if content_list and len(content_list) > 0:
                            content_encoded = content_list[0].get('value', '')
                    
                    # æ„å»ºå®Œæ•´æ–‡ç« æ•°æ®
                    article_data.append({
                        'title': title,
                        'url': article_url,
                        'date': article_date.date().isoformat(),
                        'site': site_url,
                        'description': description,
                        'summary': summary,
                        'content_encoded': content_encoded,
                        'raw_content': description + ' ' + summary + ' ' + content_encoded
                    })
        
        logger.info(f"ğŸ“… RSS[{rss_url}] æ‰¾åˆ° {len(article_data)} ç¯‡ç¬¦åˆæ—¥æœŸçš„æ–‡ç« ")
        
        # æ­¥éª¤3: ç›´æ¥ä½¿ç”¨RSSå†…å®¹ï¼Œé¿å…ç½‘é¡µæŠ“å–
        if article_data:
            for data in article_data:
                try:
                    # æ¸…ç†RSSå†…å®¹ï¼Œç§»é™¤HTMLæ ‡ç­¾å’Œå…ƒæ•°æ®
                    content_text = _clean_rss_content(data['raw_content'])
                    
                    if content_text and len(content_text) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                        articles.append({
                            'site': data['site'],
                            'url': data['url'],
                            'title': data['title'],
                            'date': data['date'],
                            'content_text': content_text
                        })
                    else:
                        logger.warning(f"RSSå†…å®¹è´¨é‡è¾ƒå·®: {data['url']} (å†…å®¹é•¿åº¦: {len(content_text)})")
                        
                except Exception as e:
                    logger.warning(f"å¤„ç†RSSå†…å®¹å¤±è´¥: {data['url']} - {str(e)}")
        
    except Exception as e:
        logger.error(f"RSSæŠ“å–å¤±è´¥ {rss_url}: {str(e)}")
    
    return articles


def _fetch_from_html(site_url: str, start_date: date, end_date: date) -> Dict:
    """
    ä»HTMLé¡µé¢æŠ“å–æ–‡ç« ï¼ˆæ”¹è¿›é”™è¯¯å¤„ç†ï¼Œä¿ç•™æ‰€æœ‰æœ‰ä»·å€¼æ•°æ®ï¼‰
    è¿”å›ï¼š{
        'articles': List[Dict],  # æˆåŠŸå¤„ç†çš„æ–‡ç« 
        'failed_articles': List[Dict],  # å¤„ç†å¤±è´¥çš„æ–‡ç« ï¼ˆä¿ç•™åŸºæœ¬ä¿¡æ¯ï¼‰
        'statistics': Dict  # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    }
    """
    result = {
        'articles': [],
        'failed_articles': [],
        'statistics': {
            'total_entries': 0,
            'filtered_by_date': 0,
            'successful_extraction': 0,
            'failed_extraction': 0,
            'error_messages': []
        }
    }
    
    # ä¸ºHTMLæŠ“å–åˆ›å»ºä¸“é—¨çš„requestsä¼šè¯ï¼Œç»•è¿‡å…¨å±€ä»£ç†æ¸…é™¤
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    
    # åˆ›å»ºæ–°çš„ä¼šè¯ï¼Œä¸ç»§æ‰¿ä¹‹å‰çš„ä»£ç†è®¾ç½®
    session = requests.Session()
    
    # å¤‡ä»½å¹¶æ¢å¤ä»£ç†ç¯å¢ƒå˜é‡
    original_env_backup = os.environ.copy()
    
    try:
        # æ¢å¤ä»£ç†è®¾ç½®ä»¥æ”¯æŒéœ€è¦VPNçš„ç½‘ç«™
        enable_proxy_for_web_scraping()
        
        # è®¾ç½®ä¼šè¯ä¿¡ä»»ç¯å¢ƒå˜é‡ï¼ˆé‡è¦ï¼ï¼‰
        session.trust_env = True
        
        # å‘é€è¯·æ±‚
        response = session.get(site_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        article_links = _extract_article_links(soup, site_url)
        result['statistics']['total_entries'] = len(article_links)
        
        for link in article_links:
            try:
                content_text = _extract_content_with_session(link, session)
                
                if content_text:
                    result['articles'].append({
                        'site': site_url,
                        'url': link,
                        'title': _extract_title_from_url(link),
                        'content_text': content_text,
                        'date': None  # HTMLæŠ“å–å¯èƒ½æ²¡æœ‰å…·ä½“æ—¥æœŸ
                    })
                    result['statistics']['successful_extraction'] += 1
                else:
                    # å†…å®¹æå–å¤±è´¥ï¼Œä½†ä¿ç•™é“¾æ¥ä¿¡æ¯
                    result['failed_articles'].append({
                        'site': site_url,
                        'url': link,
                        'title': _extract_title_from_url(link),
                        'error': 'å†…å®¹æå–å¤±è´¥',
                        'date': None
                    })
                    result['statistics']['failed_extraction'] += 1
                    
            except Exception as e:
                error_msg = f"å¤„ç†é“¾æ¥å¤±è´¥: {link} - {str(e)}"
                logger.warning(f"âš ï¸ {error_msg}")
                result['statistics']['error_messages'].append(error_msg)
                
                result['failed_articles'].append({
                    'site': site_url,
                    'url': link,
                    'title': _extract_title_from_url(link),
                    'error': str(e),
                    'date': None
                })
                result['statistics']['failed_extraction'] += 1
        
    except Exception as e:
        error_msg = f"HTMLæŠ“å–å¤±è´¥ {site_url}: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        result['statistics']['error_messages'].append(error_msg)
    
    finally:
        # å…³é—­ä¼šè¯
        session.close()
        # æ¢å¤ç¯å¢ƒå˜é‡çŠ¶æ€
        os.environ.clear()
        os.environ.update(original_env_backup)
    
    return result


def _extract_content_with_session(url: str, session: requests.Session) -> Optional[str]:
    """ä½¿ç”¨æŒ‡å®šä¼šè¯æå–å†…å®¹ï¼Œç”¨äºæ”¯æŒVPNä»£ç†"""
    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ä½¿ç”¨æŒ‡å®šä¼šè¯å‘é€è¯·æ±‚
        response = session.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            content = soup.get_text()
            if content and len(content.strip()) > 100:
                return content.strip()
                
    except Exception as e:
        logger.warning(f"åå¤‡å†…å®¹æå–å¤±è´¥: {url} - {str(e)}")
    
    return None


def _extract_article_links(soup: BeautifulSoup, base_url: str) -> List[str]:
    links = []
    
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        
        if href.startswith('/'):
            href = base_url.rstrip('/') + href
        elif not href.startswith('http'):
            continue
        
        if _is_article_link(href):
            links.append(href)
    
    return list(set(links))


def _is_article_link(url: str) -> bool:
    exclude_patterns = ['#', '/tag/', '/category/', '/author/', '/page/', 'login', 'register']
    
    for pattern in exclude_patterns:
        if pattern in url:
            return False
    
    return True


def _extract_content(url: str) -> Optional[str]:
    """
    æå–æ–‡ç« å†…å®¹ï¼ˆç®€åŒ–ä¸ºåå¤‡æ–¹æ¡ˆï¼‰
    æ³¨æ„ï¼šç°åœ¨RSSå·²æä¾›å®Œæ•´å†…å®¹ï¼Œæ­¤å‡½æ•°ä»…åœ¨RSSå†…å®¹è´¨é‡æå·®æ—¶ä½¿ç”¨
    """
    try:
        # è·å–é…ç½®çš„ä¼šè¯
        session = globals().get('_scraping_session', None)
        
        # ç®€åŒ–è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        if session:
            response = session.get(url, headers=headers, timeout=15)
        else:
            import requests
            response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            content = soup.get_text()
            if content and len(content.strip()) > 100:
                return content.strip()
                
    except Exception as e:
        logger.warning(f"åå¤‡å†…å®¹æå–å¤±è´¥: {url} - {str(e)}")
    
    return None


def _extract_title_from_url(url: str) -> str:
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
    except Exception as e:
        pass
    
    return url


def _is_english(text: str) -> bool:
    if not text:
        return False
    
    english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
    total_chars = sum(1 for char in text if char.isalpha())
    
    if total_chars == 0:
        return False
    
    return english_chars / total_chars > 0.5



def process_articles_with_ai(articles_list: List[Dict], max_workers: int = 10, batch_size: int = 3) -> str:
    """
    æ‰¹é‡å¹¶è¡Œå¤„ç†æ–‡ç« å¹¶ç”ŸæˆMarkdownï¼ˆæ”¯æŒç¼“å­˜ï¼‰
    
    Args:
        articles_list: æ–‡ç« åˆ—è¡¨
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        batch_size: æ¯ä¸ªæ‰¹é‡å¤„ç†çš„æ–‡ç« æ•°é‡ï¼ˆå»ºè®®3-5ç¯‡ï¼‰
    
    Returns:
        Markdownæ ¼å¼çš„å‘¨æŠ¥æ–‡æœ¬
    """
    if not articles_list:
        return ''
    
    # é¦–å…ˆç­›é€‰å‡ºéœ€è¦AIå¤„ç†çš„æ–‡ç« ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    cached_articles = []
    articles_to_process = []
    
    for article in articles_list:
        url = article.get('url', '')
        cached_result = load_ai_from_cache(url)
        if cached_result:
            logger.info(f"ğŸš€ AIç¼“å­˜å‘½ä¸­: {url}")
            cached_articles.append(cached_result)
        else:
            articles_to_process.append(article)
    
    logger.info(f"ğŸ“Š AIç¼“å­˜ç»Ÿè®¡: {len(cached_articles)}ç¯‡å‘½ä¸­ç¼“å­˜, {len(articles_to_process)}ç¯‡éœ€è¦AIå¤„ç†")
    
    # å¦‚æœæ‰€æœ‰æ–‡ç« éƒ½æœ‰ç¼“å­˜ï¼Œç›´æ¥ç”ŸæˆMarkdown
    if not articles_to_process:
        logger.info("âœ… æ‰€æœ‰æ–‡ç« å‡å‘½ä¸­ç¼“å­˜ï¼Œè·³è¿‡AIå¤„ç†")
        return _generate_markdown(cached_articles)
    
    try:
        client = _get_openai_client()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        # å¦‚æœæœ‰ç¼“å­˜çš„æ–‡ç« ï¼Œä»ç„¶è¿”å›ç¼“å­˜ç»“æœ
        if cached_articles:
            return _generate_markdown(cached_articles)
        return ''
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡å¹¶è¡ŒAIå¤„ç†: {len(articles_to_process)} ç¯‡æ–‡ç« ")
    logger.info(f"ğŸ¤– AIæ‰¹é‡è®¾ç½®: batch_size={batch_size}, max_workers={max_workers}")
    
    # å°†éœ€è¦å¤„ç†çš„æ–‡ç« åˆ†æ‰¹
    batches = [articles_to_process[i:i + batch_size] for i in range(0, len(articles_to_process), batch_size)]
    logger.info(f"ğŸ¤– åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†")
    
    newly_processed_articles = []
    completed = 0
    
    # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æ‰¹é‡å¤„ç†ä»»åŠ¡
        futures = []
        batch_index_map = {}  # è®°å½•futureå’Œå¯¹åº”æ‰¹æ¬¡æ•°çš„æ˜ å°„
        for i, batch in enumerate(batches, 1):
            future = executor.submit(_process_batch_with_ai, client, batch, i)
            futures.append(future)
            batch_index_map[id(future)] = i  # è®°å½•æ˜ å°„å…³ç³»
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(futures):
            try:
                batch_results = future.result()
                newly_processed_articles.extend(batch_results)
                completed += 1
                current_batch_index = batch_index_map.get(id(future), completed)
                logger.info(f"âœ… å®Œæˆ {completed}/{len(batches)} ä¸ªæ‰¹æ¬¡ (æ‰¹æ¬¡ {current_batch_index})")
            except Exception as e:
                current_batch_index = batch_index_map.get(id(future), "æœªçŸ¥")
                logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: æ‰¹æ¬¡ {current_batch_index} - {str(e)}")
                completed += 1
    
    # åˆå¹¶ç¼“å­˜ç»“æœå’Œæ–°å¤„ç†çš„ç»“æœ
    all_processed_articles = cached_articles + newly_processed_articles
    logger.info(f"ğŸ‰ æ‰¹é‡å¹¶è¡ŒAIå¤„ç†å®Œæˆ: {len(cached_articles)}ç¯‡æ¥è‡ªç¼“å­˜, {len(newly_processed_articles)}ç¯‡æ–°å¤„ç†, æ€»è®¡{len(all_processed_articles)}ç¯‡")
    
    markdown_text = _generate_markdown(all_processed_articles)
    
    return markdown_text


def _process_batch_with_ai(client: OpenAI, batch: List[Dict], batch_index: int) -> List[Dict]:
    """
    æ‰¹é‡å¤„ç†æ–‡ç« ï¼ˆä¸€æ¬¡å¤„ç†å¤šç¯‡æ–‡ç« ï¼Œæ”¯æŒç¼“å­˜ï¼‰
    
    Args:
        client: OpenAIå®¢æˆ·ç«¯
        batch: æ–‡ç« æ‰¹æ¬¡
        batch_index: æ‰¹æ¬¡ç´¢å¼•
    
    Returns:
        å¤„ç†åçš„æ–‡ç« åˆ—è¡¨
    """
    if not batch:
        return []
    
    logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_index}: {len(batch)} ç¯‡æ–‡ç« ")
    
    # é¦–å…ˆæ£€æŸ¥æ¯ç¯‡æ–‡ç« çš„ç¼“å­˜
    cached_results = []
    articles_to_process = []
    
    for article in batch:
        url = article.get('url', '')
        cached_result = load_ai_from_cache(url)
        if cached_result:
            logger.info(f"ğŸš€ AIç¼“å­˜å‘½ä¸­: {url}")
            cached_results.append((article, cached_result))
        else:
            articles_to_process.append(article)
    
    # å¦‚æœæ‰€æœ‰æ–‡ç« éƒ½æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if not articles_to_process:
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} å…¨éƒ¨å‘½ä¸­ç¼“å­˜")
        return [result for _, result in cached_results]
    
    # å¦‚æœæœ‰éƒ¨åˆ†æ–‡ç« éœ€è¦å¤„ç†ï¼Œæ„å»ºprompt
    if len(articles_to_process) < len(batch):
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {batch_index}: {len(cached_results)}ç¯‡å‘½ä¸­ç¼“å­˜, {len(articles_to_process)}ç¯‡éœ€è¦AIå¤„ç†")
    
    # æ„å»ºæ‰¹é‡å¤„ç†çš„promptï¼ˆåªå¤„ç†æœªç¼“å­˜çš„æ–‡ç« ï¼‰
    articles_info = []
    for i, article in enumerate(articles_to_process, 1):
        title = article.get('title', '')
        content_text = article.get('content_text', '')
        url = article.get('url', '')
        date_str = article.get('date', '')
        
        articles_info.append(f"""
æ–‡ç«  {i}:
æ ‡é¢˜: {title}
é“¾æ¥: {url}
æ—¥æœŸ: {date_str}
æ­£æ–‡: {content_text[:1500]}...
""")
    
    batch_content = '\n'.join(articles_info)
    
    prompt = f"""
# Role
ä½ æ˜¯ä¸€åèµ„æ·±çš„**æˆ·å¤–æé™è¿åŠ¨ç¼–è¾‘ + ä¸“æ³¨äºâ€œæˆ·å¤–æ–‡åŒ–è§‚å¯Ÿâ€å’Œâ€œå½±åƒç¾å­¦â€çš„è‡ªåª’ä½“ï¼ˆæ–‡ç« /æ’­å®¢ï¼‰**ï¼Œç²¾é€šç™»å±±ã€æ”€å²©ã€å¾’æ­¥ç­‰é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œæœ¯è¯­ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ‰¹é‡å¤„ç†å¤šç¯‡æ–‡ç« ï¼Œæå–æ¯ç¯‡æ–‡ç« çš„æ ¸å¿ƒä¿¡æ¯å¹¶ç”Ÿæˆå‘¨æŠ¥ç´ æã€‚

# Input Data
ä»¥ä¸‹æ˜¯ {len(batch)} ç¯‡æˆ·å¤–è¿åŠ¨ç›¸å…³æ–‡ç« ï¼Œè¯·é€ä¸ªåˆ†æï¼š

{batch_content}

# Goals
è¯·ä¸ºæ¯ç¯‡æ–‡ç« æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼š

å¯¹äºæ¯ç¯‡æ–‡ç« ï¼Œè¿”å›ä»¥ä¸‹ç»“æ„çš„JSONå¯¹è±¡ï¼š
{{
    "chinese_title": "å¯¹æ ‡é¢˜è¿›è¡Œä¸­æ–‡ç¿»è¯‘ï¼‰",
    "summary": "æ ¸å¿ƒäº‹ä»¶æ¦‚æ‹¬ï¼ˆäººç‰©+åœ°ç‚¹+æˆå°±ï¼‰ï¼Œè¦æ±‚ä½¿ç”¨åŸæ–‡è¯­è¨€",
    "chinese_summary": "è‹¥summaryä¸ºä¸­æ–‡åˆ™èµ‹å€¼ä¸ºsummaryï¼›å¦åˆ™ï¼Œå¯¹summaryè¿›è¡Œä¸­æ–‡ç¿»è¯‘", 
    "key_persons": ["å…³é”®äººç‰©1", "å…³é”®äººç‰©2"],
    "location": "äº‹ä»¶åœ°ç‚¹ï¼Œä½¿ç”¨åŸæ–‡ã€‚æ— åˆ™è¿”å›ç©º",
    "event_date": "äº‹ä»¶æ—¶é—´",
    "key_person_bios": {{
        "ç›¸å…³äººç‰©è‹±æ–‡åŸå": "ä¸€å¥è¯ä¸­æ–‡æ·±åº¦ç®€ä»‹ï¼ˆèƒŒæ™¯ã€æˆå°±ã€é£æ ¼ï¼‰"
    }},
    "location_context": "äº‹ä»¶åœ°ç‚¹ä»‹ç»",
    "curated_angles": {{
        "é€‰é¢˜è§’åº¦1": "é€‰é¢˜å†…å®¹"
    }}
}}

# Output Format
ç¿»è¯‘æ—¶ï¼Œæ³¨æ„æˆ·å¤–è¿åŠ¨ä¸“ä¸šæœ¯è¯­çš„ç¿»è¯‘
å¿…é¡»è¿”å›çº¯å‡€çš„JSONæ•°ç»„æ ¼å¼ï¼Œä¸¥ç¦ä½¿ç”¨Markdownä»£ç å—ã€‚
key_personsï¼Œä½¿ç”¨åŸæ–‡äººåï¼Œä¸å¾—è¿›è¡Œç¿»è¯‘
key_person_biosï¼Œè¦æ±‚å¯¹key_personsçš„æ¯ä¸ªäººç‰©ï¼Œç”¨ä¸€å¥è¯ä¸­æ–‡è¿›è¡Œç®€ä»‹ï¼ˆèƒŒæ™¯ã€æˆå°±ã€é£æ ¼ï¼‰
location_contextï¼šå¦‚æœæ²¡æœ‰äº‹ä»¶åœ°ç‚¹åˆ™ä¸ºç©ºã€‚å¦‚æœäº‹ä»¶åœ°ç‚¹æ˜¯å±±å³°æˆ–æ”€å²©çº¿è·¯ï¼Œå¿…é¡»è¡¥å……å…¶æ”€ç™»å†å²ã€é¦–æ”€ä¿¡æ¯ä»¥åŠéš¾åº¦ç­‰çº§ç­‰ï¼›å¦‚æœæ˜¯æ™®é€šåœ°ç‚¹ï¼Œè¡¥å……å…¶åœ°ç†æˆ–æˆ·å¤–æ–‡åŒ–èƒŒæ™¯ã€‚",
curated_anglesï¼šè¯·ä¸ºç”¨æˆ·ç”Ÿæˆ3ä¸ªæ·±åº¦é€‰é¢˜è§’åº¦ã€‚
   - **æ€è€ƒç»´åº¦**ï¼šè¯·ä»â€œå½±åƒç¾å­¦â€ã€â€œæ¢é™©ä¼¦ç†â€ã€â€œå•†ä¸šä¸çº¯ç²¹çš„å†²çªâ€ã€â€œäººç‰©å†…å¿ƒâ€ã€â€œæé™è¿åŠ¨çš„ç¤¾ä¼šéšå–»â€ç­‰è§’åº¦å‘æ•£ã€‚
   - **æ ¼å¼è¦æ±‚**ï¼šæ¯ä¸ªè§’åº¦è¯·ç”¨ã€æ ‡ç­¾ã€‘ï¼šå…·ä½“æè¿°çš„å½¢å¼ã€‚
   - **ç¤ºä¾‹**ï¼š
     - "å½±åƒåˆ†æï¼šåˆ†ææ‘„å½±å¸ˆ Jimmy Chin å¦‚ä½•åˆ©ç”¨å¹¿è§’é•œå¤´è¡¨ç° Meru é²¨é±¼é³çš„å‹è¿«æ„Ÿ"
     - "æ–‡åŒ–è§‚å¯Ÿï¼šä»è¿™æ¬¡å•†ä¸šç™»å±±äº‹æ•…ï¼Œçœ‹â€˜ä¿å§†å¼ç™»å±±â€™å¯¹é˜¿è‚¯è‰²å·æ¢é™©æ–‡åŒ–çš„ä¾µèš€"
     - "æ’­å®¢è¯é¢˜ï¼šå½“èµåŠ©å•†è¦æ±‚â€˜å¿…é¡»ç™»é¡¶â€™æ—¶ï¼Œæ”€ç™»è€…çš„å¿ƒç†åšå¼ˆ"

"""
    
    try:
        model_name = os.getenv('LLM_MODEL')
        if not model_name:
            raise ValueError('LLM_MODEL environment variable is not set')

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ·å¤–æ–°é—»æ–¹å‘çš„æ–‡ç« åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿æ‰¹é‡æå–æ–‡ç« å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œä¸­è‹±æ–‡ç¿»è¯‘ã€‚'},
                {'role': 'user', 'content': prompt}
            ],
            temperature=0.3,
            response_format={'type': 'json_object'},
            timeout=60  # æ‰¹é‡å¤„ç†éœ€è¦æ›´é•¿æ—¶é—´
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # è§£æJSONç»“æœ
        import json
        results = json.loads(result_text)
        
        # ç¡®ä¿ç»“æœæ˜¯æ•°ç»„æ ¼å¼
        if isinstance(results, dict) and 'articles' in results:
            results = results['articles']
        elif not isinstance(results, list):
            logger.warning(f"æ‰¹æ¬¡ {batch_index} è¿”å›æ ¼å¼å¼‚å¸¸ï¼Œå°è¯•ä¿®å¤...")
            results = [results] if not isinstance(results, list) else results
        
        # å°†ç»“æœæ˜ å°„å›åŸå§‹æ–‡ç« æ•°æ®
        # å¤„ç†AIè¿”å›çš„ç»“æœ
        newly_processed = []
        for i, result in enumerate(results):
            if i < len(articles_to_process):  # ç¡®ä¿ä¸è¶…è¿‡éœ€è¦å¤„ç†çš„æ–‡ç« æ•°é‡
                article = articles_to_process[i]
                
                # ç¡®ä¿resultæ˜¯å­—å…¸æ ¼å¼
                if not isinstance(result, dict):
                    logger.warning(f"æ‰¹æ¬¡ {batch_index} æ–‡ç«  {i+1} ç»“æœæ ¼å¼å¼‚å¸¸: {type(result)}")
                    result = {'chinese_title': article.get('title', '')}
                
                processed_article = {
                    'original_title': article.get('title', ''),
                    'chinese_title': result.get('chinese_title', article.get('title', '')),
                    'summary': result.get('summary', article.get('content_text', '')[:200] + '...'),
                    'chinese_summary': result.get('chinese_summary', result.get('summary', article.get('content_text', '')[:200] + '...')),
                    'key_persons': result.get('key_persons', []),
                    'key_person_bios': result.get('key_person_bios', {}),
                    'location': result.get('location', 'æœªçŸ¥åœ°ç‚¹'),
                    'location_context': result.get('location_context', ''),
                    'curated_angles': result.get('curated_angles', []),
                    'url': article.get('url', ''),
                    'date': article.get('date', ''),
                    'site': article.get('site', ''),
                    'ai_processed_at': datetime.now().isoformat(),
                    'content_length': len(article.get('content_text', ''))
                }
                newly_processed.append(processed_article)
        
        # åˆå¹¶ç¼“å­˜ç»“æœå’Œæ–°å¤„ç†çš„ç»“æœ
        all_processed = [result for _, result in cached_results] + newly_processed
        
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_index} å¤„ç†æˆåŠŸ: {len(cached_results)}ç¯‡æ¥è‡ªç¼“å­˜, {len(newly_processed)}ç¯‡æ–°å¤„ç†")
        
        # åªä¸ºæ–°å¤„ç†çš„æ–‡ç« ä¿å­˜AIç¼“å­˜
        for processed_article in newly_processed:
            url = processed_article.get('url', '')
            if url:
                save_ai_to_cache(url, processed_article)
        
        return all_processed
        
    except Exception as e:
        logger.error(f"æ‰¹æ¬¡ {batch_index} AIå¤„ç†å¤±è´¥: {str(e)}")
        
        # å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
        processed = []
        for article in batch:
            processed.append({
                'original_title': article.get('title', ''),
                'chinese_title': article.get('title', ''),
                'summary': article.get('content_text', '')[:200] + '...',
                'chinese_summary': article.get('content_text', '')[:200] + '...',
                'key_persons': [],
                'location': 'æœªçŸ¥åœ°ç‚¹',
                'event_date': '',
                'url': article.get('url', ''),
                'date': article.get('date', ''),
                'site': article.get('site', ''),
                'ai_processed_at': datetime.now().isoformat(),
                'content_length': len(article.get('content_text', '')),
                'error': str(e)
            })
        
        # å³ä½¿å¤±è´¥ä¹Ÿä¿å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤å¤„ç†
        for processed_article in processed:
            url = processed_article.get('url', '')
            if url:
                save_ai_to_cache(url, processed_article)
        
        return processed


def _generate_markdown(articles: List[Dict]) -> str:
    if not articles:
        return ''
    
    markdown_lines = []
    markdown_lines.append('# æˆ·å¤–è¿åŠ¨å‘¨æŠ¥\n')
    markdown_lines.append(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
    markdown_lines.append(f'å…±æ”¶å½• {len(articles)} ç¯‡æ–‡ç« \n')
    
    # æå–æœç´¢çš„ç½‘ç«™åˆ—è¡¨
    source_sites = list(set(article.get('site') for article in articles if isinstance(article, dict) and article.get('site')))
    if source_sites:
        markdown_lines.append('\n## æœç´¢æ¥æºç½‘ç«™\n')
        for site in source_sites:
            markdown_lines.append(f'- {site}\n')
        markdown_lines.append('\n---\n')
    
    for i, article in enumerate(articles, 1):
        markdown_lines.append(f'\n## {i}. {article["chinese_title"]}\n')
        
        if article.get('original_title') and article.get('original_title') != article.get('chinese_title'):
            markdown_lines.append(f'**åŸæ ‡é¢˜**: {article["original_title"]}\n')
        
        if article.get('date'):
            markdown_lines.append(f'**æ—¥æœŸ**: {article["date"]}\n')
        
        markdown_lines.append(f'**é“¾æ¥**: {article["url"]}\n')
        
        if article.get('key_persons'):
            persons_text = 'ã€'.join(article['key_persons'])
            markdown_lines.append(f'**å…³é”®äººç‰©**: {persons_text}\n')
            
            # ä¸ºæ¯ä¸ªå…³é”®äººç‰©ç”Ÿæˆæœç´¢é“¾æ¥
            for person in article['key_persons']:
                person_encoded = person.replace(' ', '+')
                search_url = f"https://www.google.com/search?q={person_encoded}+outdoor"
                markdown_lines.append(f'- [{person}]({search_url})\n')
            
            if article.get('key_person_bios'):
                for person_name, bio_text in article['key_person_bios'].items():
                    markdown_lines.append(f'  - **{person_name}**: {bio_text}\n')
            else:
                markdown_lines.append(f'  - **äººç‰©ç®€ä»‹**: æ— \n')
        else:
            markdown_lines.append(f'**å…³é”®äººç‰©**: æ— \n')
        
        if article.get('location_context'):
            markdown_lines.append(f'**åœ°ç‚¹èƒŒæ™¯ä¸å†å²**: {article["location_context"]}\n')
        else:
            markdown_lines.append(f'**åœ°ç‚¹èƒŒæ™¯ä¸å†å²**: æ— \n')
        
        if article.get('curated_angles'):
            angles = article['curated_angles']
            if isinstance(angles, dict):
                angles_list = list(angles.values())
            else:
                angles_list = angles
            markdown_lines.append(f'**é€‰é¢˜ç­–åˆ’è§’åº¦**:\n')
            for angle in angles_list:
                markdown_lines.append(f'  - {angle}\n')
        else:
            markdown_lines.append(f'**é€‰é¢˜ç­–åˆ’è§’åº¦**: æ— \n')
        
        markdown_lines.append(f'\n**æ‘˜è¦**: {article["summary"]}\n')
        
        if article.get('chinese_summary') and article.get('chinese_summary') != article.get('summary'):
            markdown_lines.append(f'\n*ä¸­æ–‡æ‘˜è¦*: {article["chinese_summary"]}\n')
        
        markdown_lines.append('\n---\n')
    
    return ''.join(markdown_lines)


def _parse_text_with_links(text):
    """
    [å†…éƒ¨å·¥å…·] è§£æåŒ…å« Markdown é“¾æ¥çš„æ–‡æœ¬
    è¾“å…¥: "ç‚¹å‡» [è¿™é‡Œ](http://google.com) æŸ¥çœ‹"
    è¾“å‡º: é£ä¹¦ TextElement ç»“æ„æ•°ç»„
    """
    elements = []
    # æ­£åˆ™åŒ¹é… [text](url)
    pattern = re.compile(r'\[(.*?)\]\((.*?)\)')
    last_idx = 0
    
    for match in pattern.finditer(text):
        # 1. æ·»åŠ é“¾æ¥å‰çš„æ™®é€šæ–‡æœ¬
        if match.start() > last_idx:
            elements.append(TextElement(
                text_run=TextRun(content=text[last_idx:match.start()])
            ))
        
        # 2. æ·»åŠ é“¾æ¥æ–‡æœ¬
        link_text = match.group(1)
        link_url = match.group(2)
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(link_text)
                .text_element_style(TextElementStyle.builder()
                    .link(Link.builder().url(link_url).build())
                    .build())
                .build())
            .build())
        last_idx = match.end()
    
    # 3. æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
    if last_idx < len(text):
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text[last_idx:])
                .build())
            .build())
        
    # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥è¿”å›çº¯æ–‡æœ¬
    if not elements:
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text)
                .build())
            .build())
        
    return elements

def publish_feishu_report(report_title, markdown_content, chat_id):
    """
    æ ¸å¿ƒåŠŸèƒ½: åˆ›å»ºæ–‡æ¡£ -> å†™å…¥å†…å®¹ -> å‘é€å¡ç‰‡
    """
    print(f"ğŸš€ [Feishu] å‡†å¤‡å‘å¸ƒæ–‡æ¡£: {report_title}")
    
    # è·å–é£ä¹¦å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¸…é™¤ä»£ç†ï¼‰
    client = get_feishu_client()
    
    # =================================================
    # æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºç™½æ–‡æ¡£
    # =================================================
    try:
        create_req = CreateDocumentRequest.builder() \
            .request_body(CreateDocumentRequestBody.builder()
                .title(report_title)
                .build()) \
            .build()
            
        resp = client.docx.v1.document.create(create_req)
        
        if not resp.success():
            print(f"âŒ åˆ›å»ºæ–‡æ¡£å¤±è´¥: {resp.code} - {resp.msg}")
            return None
            
        document_id = resp.data.document.document_id
        # æ³¨æ„: åªæœ‰é£ä¹¦å›½å†…ç‰ˆæ˜¯ feishu.cnï¼Œå›½é™…ç‰ˆè¯·æ”¹ä¸º larksuite.com
        doc_url = f"https://feishu.cn/docx/{document_id}"
        print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")

        collaborator_openids = os.getenv("FEISHU_COLLABORATOR_OPENIDS", "")
        collaborator_perm = os.getenv("FEISHU_COLLABORATOR_PERM", "edit")
        
        if collaborator_openids:
            openids = [oid.strip() for oid in collaborator_openids.split(",") if oid.strip()]
            
            added_count = 0
            failed_count = 0
            
            for openid in openids:
                try:
                    add_req = CreatePermissionMemberRequest.builder() \
                        .token(document_id) \
                        .type("docx") \
                        .need_notification(False) \
                        .request_body(BaseMember.builder()
                            .member_type("openid")
                            .member_id(openid)
                            .perm(collaborator_perm)
                            .perm_type("container")
                            .type("user")
                            .build()) \
                        .build()
                    
                    add_resp = client.drive.v1.permission_member.create(add_req)
                    
                    if add_resp.success():
                        print(f"âœ… åä½œè€…æ·»åŠ æˆåŠŸ: {openid}")
                        added_count += 1
                    else:
                        print(f"âš ï¸ åä½œè€…æ·»åŠ å¤±è´¥: {openid} - {add_resp.msg}")
                        failed_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ ä¸º {openid} æ·»åŠ åä½œè€…æ—¶å‡ºé”™: {e}")
                    failed_count += 1
            
            if added_count > 0:
                print(f"âœ… æˆåŠŸæ·»åŠ  {added_count} ä¸ªåä½œè€…ï¼Œæƒé™: {collaborator_perm}")
            if failed_count > 0:
                print(f"âš ï¸ {failed_count} ä¸ªåä½œè€…æ·»åŠ å¤±è´¥")

    except Exception as e:
        print(f"âŒ é£ä¹¦ API è¿æ¥é”™è¯¯: {e}")
        return None

    # =================================================
    # æ­¥éª¤ 2: ä½¿ç”¨é£ä¹¦å®˜æ–¹ API å°† Markdown è½¬æ¢ä¸º Blocks
    # =================================================
    print("ğŸ”„ æ­£åœ¨å°† Markdown è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£å—...")
    
    # è°ƒç”¨é£ä¹¦å®˜æ–¹çš„ Markdown è½¬æ¢ API
    convert_req = ConvertDocumentRequest.builder() \
        .request_body(ConvertDocumentRequestBody.builder()
            .content_type("markdown")
            .content(markdown_content)
            .build()) \
        .build()
    
    convert_resp = client.docx.v1.document.convert(convert_req)
    
    if not convert_resp.success():
        print(f"âŒ Markdown è½¬æ¢å¤±è´¥: {convert_resp.code} - {convert_resp.msg}")
        return None
    
    # è·å–è½¬æ¢åçš„ blocks
    blocks = convert_resp.data.blocks
    first_level_block_ids = convert_resp.data.first_level_block_ids or []
    
    if not blocks:
        print("âš ï¸ è½¬æ¢åçš„å†…å®¹ä¸ºç©º")
        return doc_url
    
    # ä½¿ç”¨ first_level_block_ids é‡æ–°æ’åº blocks
    if first_level_block_ids:
        block_map = {b.block_id: b for b in blocks}
        ordered_blocks = []
        for block_id in first_level_block_ids:
            if block_id in block_map:
                ordered_blocks.append(block_map[block_id])
        # æ·»åŠ ä¸åœ¨ first_level_block_ids ä¸­çš„ blocks
        for block in blocks:
            if block.block_id not in first_level_block_ids:
                ordered_blocks.append(block)
        blocks = ordered_blocks
    
    print(f"âœ… Markdown è½¬æ¢æˆåŠŸï¼Œå…± {len(blocks)} ä¸ª blocks")
    
    # =================================================
    # æ­¥éª¤ 3: ä½¿ç”¨è½¬æ¢å¥½çš„ blocks å†™å…¥æ–‡æ¡£å†…å®¹
    # =================================================
    print("ğŸ“ æ­£åœ¨å†™å…¥æ–‡æ¡£å†…å®¹...")
    
    try:
        # å°† blocks åˆ†æ‰¹å†™å…¥ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
        batch_size = 50
        for i in range(0, len(blocks), batch_size):
            batch = blocks[i:i + batch_size]
            
            # ç›´æ¥ä½¿ç”¨è½¬æ¢å¥½çš„ block å¯¹è±¡
            batch_req = CreateDocumentBlockChildrenRequest.builder() \
                .document_id(document_id) \
                .block_id(document_id) \
                .request_body(CreateDocumentBlockChildrenRequestBody.builder()
                    .children(batch)
                    .build()) \
                .build()
            
            batch_resp = client.docx.v1.document_block_children.create(batch_req)
            
            if not batch_resp.success():
                print(f"âš ï¸ æ‰¹æ¬¡å†™å…¥å¤±è´¥ (æ‰¹æ¬¡ {i//batch_size + 1}): {batch_resp.code} - {batch_resp.msg}")
            else:
                print(f"âœ… æ‰¹æ¬¡å†™å…¥æˆåŠŸ (æ‰¹æ¬¡ {i//batch_size + 1}): {len(batch)} ä¸ª blocks")
        
        print(f"âœ… æ–‡æ¡£å†…å®¹å†™å…¥å®Œæˆï¼Œå…± {len(blocks)} ä¸ª blocks")
            
    except Exception as e:
        print(f"âš ï¸ å†™å…¥æ–‡æ¡£å†…å®¹æ—¶å‡ºé”™: {e}")
        print("ğŸ“ è·³è¿‡å†…å®¹å†™å…¥ï¼Œç»§ç»­å‘é€é€šçŸ¥...")
        # å³ä½¿å‡ºé”™ï¼Œä¹Ÿç»§ç»­åç»­æ­¥éª¤

    # =================================================
    # æ­¥éª¤ 4: å‘é€å¯Œæ–‡æœ¬å¡ç‰‡æ¶ˆæ¯
    # =================================================
    print(f"ğŸ“¤ æ­£åœ¨æ¨é€åˆ°ç¾¤ç»„: {chat_id}")
    
    # æ„é€ å¡ç‰‡ JSON
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": "ğŸ§—â€â™‚ï¸ æˆ·å¤–èµ„è®¯å‘¨æŠ¥å·²ç”Ÿæˆ"},
            "template": "blue" # æ ‡é¢˜èƒŒæ™¯è‰²: blue, wathet, turquoise, green, yellow, orange, red, carmine, violet, purple, indigo, grey
        },
        "elements": [
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"æœ¬å‘¨èµ„è®¯å·²ç”± AI æ•´ç†å®Œæ¯•ã€‚\n**æ ‡é¢˜ï¼š** {report_title}\n**æ—¶é—´ï¼š** {os.getenv('TODAY', 'æœ¬å‘¨')}"
                }
            },
            {
                "tag": "hr" # åˆ†å‰²çº¿
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ‘‰ ç‚¹å‡»é˜…è¯»å®Œæ•´å‘¨æŠ¥"},
                        "url": doc_url,
                        "type": "primary"
                    }
                ]
            }
        ]
    }

    # å‘é€è¯·æ±‚
    msg_req = CreateMessageRequest.builder() \
        .receive_id_type("chat_id") \
        .request_body(CreateMessageRequestBody.builder() \
            .receive_id(chat_id) \
            .msg_type("interactive") \
            .content(json.dumps(card_content)) \
            .build()) \
        .build()
    # æµ‹è¯•éœ€è¦ï¼Œæš‚æ—¶æ³¨é‡Šå‘é€é£ä¹¦ç¾¤ç»„ä»£ç 
    # try:
    #     msg_resp = client.im.v1.message.create(msg_req)
        
    #     if msg_resp.success():
    #         print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
    #     else:
    #         print(f"âš ï¸ æ¶ˆæ¯æ¨é€å¤±è´¥: {msg_resp.code} - {msg_resp.msg}")
    #         print("ğŸ“ ä»ç„¶è¿”å›æ–‡æ¡£URL...")
    # except Exception as e:
    #     print(f"âš ï¸ å‘é€æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
    #     print("ğŸ“ ä»ç„¶è¿”å›æ–‡æ¡£URL...")
    
    # å…³é”®ï¼šå§‹ç»ˆè¿”å›æ–‡æ¡£URLï¼Œå³ä½¿å†…å®¹å†™å…¥æˆ–æ¶ˆæ¯æ¨é€å¤±è´¥
    print(f"ğŸ‰ é£ä¹¦æ–‡æ¡£å‘å¸ƒå®Œæˆ!")
    print(f"ğŸ“„ æ–‡æ¡£é“¾æ¥: {doc_url}")
    return doc_url
